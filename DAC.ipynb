{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "257fec39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 1] : Data Preparation\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "[Phase 2] : Model setup\n",
      "| Building net\n",
      "| Building net type [vggnet]...\n",
      "using dac loss function\n",
      "\n",
      "\n",
      "[Phase 3] : Training model\n",
      "| Training Epochs = 10\n",
      "| Initial Learning Rate = 0.001\n",
      "\n",
      "=> Training Epoch #1, LR=0.0010\n",
      "| Epoch [  1/ 10] Iter[391/391]\t\tAbstained 0 Abstention rate 0.0000 Cumulative Abstention Rate: 0.0000 Loss: 0.0167 Acc@1: 99.540% Acc@2: 99.540%\n",
      "| Validation Epoch #1\t\t\tAbstained: 0 Loss: 0.3308 Acc@1: 92.04% Acc@2: 92.04% \n",
      "| Elapsed time : 0:00:20\n",
      "\n",
      "=> Training Epoch #2, LR=0.0010\n",
      "| Epoch [  2/ 10] Iter[391/391]\t\tAbstained 0 Abstention rate 0.0000 Cumulative Abstention Rate: 0.0000 Loss: 0.0026 Acc@1: 99.604% Acc@2: 99.604%0%\n",
      "| Validation Epoch #2\t\t\tAbstained: 0 Loss: 0.3325 Acc@1: 92.03% Acc@2: 92.03% \n",
      "| Elapsed time : 0:00:41\n",
      "\n",
      "=> Training Epoch #3, LR=0.0010\n",
      "| Epoch [  3/ 10] Iter[391/391]\t\tAbstained 0 Abstention rate 0.0000 Cumulative Abstention Rate: 0.0000 Loss: 0.0246 Acc@1: 99.562% Acc@2: 99.562%\n",
      "| Validation Epoch #3\t\t\tAbstained: 0 Loss: 0.3325 Acc@1: 92.09% Acc@2: 92.09% \n",
      "| Elapsed time : 0:01:02\n",
      "\n",
      "=> Training Epoch #4, LR=0.0010\n",
      "| Epoch [  4/ 10] Iter[391/391]\t\tAbstained 0 Abstention rate 0.0000 Cumulative Abstention Rate: 0.0000 Loss: 0.0041 Acc@1: 99.642% Acc@2: 99.642%0%\n",
      "| Validation Epoch #4\t\t\tAbstained: 0 Loss: 0.3418 Acc@1: 92.00% Acc@2: 92.00% \n",
      "| Elapsed time : 0:01:23\n",
      "\n",
      "=> Training Epoch #5, LR=0.0010\n",
      "| Epoch [  5/ 10] Iter[391/391]\t\tAbstained 0 Abstention rate 0.0000 Cumulative Abstention Rate: 0.0000 Loss: 0.0026 Acc@1: 99.634% Acc@2: 99.634%\n",
      "| Validation Epoch #5\t\t\tAbstained: 0 Loss: 0.3405 Acc@1: 92.13% Acc@2: 92.13% \n",
      "| Elapsed time : 0:01:44\n",
      "\n",
      "=> Training Epoch #6, LR=0.0010\n",
      "| Epoch [  6/ 10] Iter[391/391]\t\tAbstained 0 Abstention rate 0.0000 Cumulative Abstention Rate: 0.0000 Loss: 0.0373 Acc@1: 99.630% Acc@2: 99.630%\n",
      "| Validation Epoch #6\t\t\tAbstained: 0 Loss: 0.3438 Acc@1: 91.85% Acc@2: 91.85% \n",
      "| Elapsed time : 0:02:05\n",
      "\n",
      "=> Training Epoch #7, LR=0.0010\n",
      "| Epoch [  7/ 10] Iter[391/391]\t\tAbstained 0 Abstention rate 0.0000 Cumulative Abstention Rate: 0.0000 Loss: 0.0033 Acc@1: 99.606% Acc@2: 99.606%0%\n",
      "| Validation Epoch #7\t\t\tAbstained: 0 Loss: 0.3390 Acc@1: 92.03% Acc@2: 92.03% \n",
      "| Elapsed time : 0:02:25\n",
      "\n",
      "=> Training Epoch #8, LR=0.0010\n",
      "| Epoch [  8/ 10] Iter[391/391]\t\tAbstained 0 Abstention rate 0.0000 Cumulative Abstention Rate: 0.0000 Loss: 0.0196 Acc@1: 99.626% Acc@2: 99.626%\n",
      "| Validation Epoch #8\t\t\tAbstained: 0 Loss: 0.3450 Acc@1: 91.91% Acc@2: 91.91% \n",
      "| Elapsed time : 0:02:46\n",
      "\n",
      "=> Training Epoch #9, LR=0.0010\n",
      "| Epoch [  9/ 10] Iter[391/391]\t\tAbstained 0 Abstention rate 0.0000 Cumulative Abstention Rate: 0.0000 Loss: 0.0023 Acc@1: 99.654% Acc@2: 99.654%\n",
      "| Validation Epoch #9\t\t\tAbstained: 0 Loss: 0.3441 Acc@1: 91.89% Acc@2: 91.89% \n",
      "| Elapsed time : 0:03:07\n",
      "\n",
      "=> Training Epoch #10, LR=0.0010\n",
      "| Epoch [ 10/ 10] Iter[391/391]\t\tAbstained 0 Abstention rate 0.0000 Cumulative Abstention Rate: 0.0000 Loss: 0.0021 Acc@1: 99.648% Acc@2: 99.648%\n",
      "| Validation Epoch #10\t\t\tAbstained: 0 Loss: 0.3465 Acc@1: 91.91% Acc@2: 91.91% \n",
      "| Elapsed time : 0:03:28\n",
      "\n",
      "[Phase 4] : Testing model\n",
      "* Test results : Acc@1 = 92.13%\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-7\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='PyTorch training for deep abstaining classifiers',\n",
    "#     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "class argparser():\n",
    "    def __init__(self,k_p=0.1,k_i=0.1,k_d=0.05,abst_rate=None,output_path='./',expt_name='test_exp',\n",
    "                 save_best_model=False,save_val_scores=False,\n",
    "                 data_parallel=False,nesterov=False,resume=False,eval_model=None,use_gpu=False,\n",
    "                 save_train_scores=False,exclude_train_indices=None,label_noise_info=None,\n",
    "                 rand_labels=None,alpha_init_factor=64,alpha_final=1.0,loss_fn='dac_loss',depth=16,\n",
    "                 learn_epochs=10,test_batch_size=128,batch_size=128,dataset='cifar10',dropout=0.2,net_type='vggnet',lr=0.001):\n",
    "        self.lr=lr\n",
    "        self.net_type=net_type\n",
    "        self.dropout=dropout\n",
    "        self.dataset=dataset\n",
    "        self.batch_size=batch_size\n",
    "        self.test_batch_size=test_batch_size\n",
    "        self.learn_epochs=learn_epochs\n",
    "        self.depth=depth\n",
    "        self.loss_fn=loss_fn\n",
    "        self.alpha_final=alpha_final\n",
    "        self.alpha_init_factor=alpha_init_factor\n",
    "        self.rand_labels=rand_labels\n",
    "        self.label_noise_info=label_noise_info\n",
    "        self.exclude_train_indices=exclude_train_indices\n",
    "        self.save_train_scores=save_train_scores\n",
    "        self.use_gpu=use_gpu\n",
    "        self.eval_model=eval_model\n",
    "        self.resume=resume\n",
    "        self.nesterov=nesterov\n",
    "        self.data_parallel=data_parallel\n",
    "        self.save_val_scores=save_val_scores\n",
    "        self.save_best_model=save_best_model\n",
    "        self.expt_name=expt_name\n",
    "        self.output_path=output_path\n",
    "        self.abst_rate=abst_rate\n",
    "        self.k_p=k_p\n",
    "        self.k_i=k_i\n",
    "        self.k_d=k_d\n",
    "        \n",
    "\n",
    "\n",
    "args = argparser()\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from torch.autograd import Variable\n",
    "from utils import gpu_utils,  label_noise\n",
    "import pdb\n",
    "import numpy as np\n",
    "from networks import wide_resnet,lenet,vggnet, resnet, resnet2, cnn\n",
    "from networks import config as cf\n",
    "\n",
    "#import dac_loss_pid\n",
    "#import dac_loss\n",
    "\n",
    "from loss_functions import loss_fn_dict\n",
    "\n",
    "try:\n",
    "    import cPickle as cp\n",
    "except ModuleNotFoundError: #no cPickle in python 3\n",
    "    import pickle as cp\n",
    "\n",
    "#do time compression or dilation\n",
    "epochs=10\n",
    "epdl=1.0\n",
    "learn_epochs=0\n",
    "epochs = int(epochs*epdl)\n",
    "learn_epochs = int(learn_epochs*epdl)\n",
    "\n",
    "save_epoch_model=None\n",
    "if not save_epoch_model is None:\n",
    "    save_epoch_model = int(save_epoch_model*epdl)\n",
    "\n",
    "log_file=None\n",
    "if not log_file is None:\n",
    "    sys.stdout = open(log_file,'w')\n",
    "    sys.stderr = sys.stdout\n",
    "\n",
    "seed=1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "batch_size=128\n",
    "test_batch_size=128\n",
    "start_epoch, num_epochs = 1, epochs\n",
    "batch_size = batch_size\n",
    "best_acc = 0.\n",
    "\n",
    "print('\\n[Phase 1] : Data Preparation')\n",
    "if args.dataset == 'cifar10':\n",
    "    trainset, testset, num_classes = get_CIFAR10(root=\"./\")\n",
    "elif args.dataset == 'svhn':\n",
    "    trainset, testset, num_classes = get_SVHN()\n",
    "sys.stdout.flush()\n",
    "#abstain class id is the last class\n",
    "abstain_class_id = num_classes\n",
    "#simulate label noise if needed\n",
    "trainset = label_noise.label_noise(args, trainset, num_classes)\n",
    "#set data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "if args.save_train_scores:\n",
    "    train_perf_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "# GPU specific stuff. \n",
    "# TODO: move to gpu_utils\n",
    "use_cuda=True\n",
    "cuda_device=None\n",
    "if args.use_gpu:\n",
    "    if not args.data_parallel:\n",
    "        #keep trying to get a GPU if use GPU is specified\n",
    "        while(cuda_device is None):\n",
    "            cuda_device = gpu_utils.get_cuda_device(args)\n",
    "            use_cuda = True\n",
    "    else: #data parallel training\n",
    "        if args.parallel_device_count is None:\n",
    "            cuda_devices = gpu_utils.get_free_gpu_list(torch.cuda.device_count())\n",
    "            num_devices = len(cuda_devices)\n",
    "        else:\n",
    "            num_devices = args.parallel_device_count\n",
    "            cuda_devices = gpu_utils.get_free_gpu_list(torch.cuda.device_count())[0:num_devices]\n",
    "\n",
    "        if len(cuda_devices) == 0:\n",
    "            print(\"No free GPUs, exitting\")\n",
    "            exit()\n",
    "\n",
    "        cuda_device = cuda_devices[0]\n",
    "\n",
    "        if len(cuda_devices) < num_devices:\n",
    "            print(\"Warning: Specified number of GPus to use is %d but only %d available\" %(num_devices,len(cuda_devices)))\n",
    "        if len(cuda_devices) == 1:\n",
    "            print(\"warning: data parallel requested, but only 1 free GPU available\")\n",
    "        use_cuda = True\n",
    "        print(\"Using GPUs %s\" %(cuda_devices))\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "#only evaluate model and output softmaxes on train and test set\n",
    "if  args.eval_model is not None:\n",
    "    print('\\n[Evaluation only] : Model setup')\n",
    "    net = torch.load(args.eval_model, map_location=lambda storage, loc: storage )['net']\n",
    "    if use_cuda:\n",
    "        net = net.cuda(cuda_device)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    net.eval()\n",
    "    expt_name = str(args.expt_name) if args.expt_name is not None else \"\"\n",
    "    expt_name = \"_\"+expt_name\n",
    "\n",
    "    train_softmax_scores = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_perf_loader):\n",
    "        print(\"train batch %s\" %(batch_idx))\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(cuda_device), targets.cuda(cuda_device) # GPU settings\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)               # Forward Propagation\n",
    "        p_out = F.softmax(outputs,dim=1)\n",
    "        train_softmax_scores.append(p_out.data)\n",
    "\n",
    "    train_scores = torch.cat(train_softmax_scores).cpu().numpy()\n",
    "    print('Saving train softmax scores in evaluation mode to %s' %(os.path.basename(args.eval_model)+\".train_scores_eval\"))\n",
    "    np.save(os.path.basename(args.eval_model)+expt_name+\".train_scores_eval\", train_scores)\n",
    "\n",
    "    test_softmax_scores=[]\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        print(\"test batch %s\" %(batch_idx))\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(cuda_device), targets.cuda(cuda_device) # GPU settings\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)               # Forward Propagation\n",
    "        p_out = F.softmax(outputs,dim=1)\n",
    "        test_softmax_scores.append(p_out.data)\n",
    "\n",
    "    test_scores = torch.cat(test_softmax_scores).cpu().numpy()\n",
    "    print('Saving validation softmax scores in evaluation mode to %s' %(os.path.basename(args.eval_model)+\".val_scores_eval\"))\n",
    "    np.save(os.path.basename(args.eval_model)+expt_name+\".val_scores_eval\", test_scores)\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "\n",
    "def getNetwork(args):\n",
    "    if args.loss_fn is None:\n",
    "        extra_class = 0\n",
    "    else:\n",
    "        extra_class = 1\n",
    "\n",
    "    if (args.net_type == 'lenet'):\n",
    "        net = lenet.LeNet(num_classes+extra_class)\n",
    "        file_name = 'lenet'\n",
    "        net.apply(lenet.conv_init)\n",
    "\n",
    "    elif (args.net_type == 'vggnet'):\n",
    "        #net = vggnet.VGG(args.depth, num_classes+extra_class, args.dropout)\n",
    "        \n",
    "        net = vggnet.VGG(args.depth, num_classes+extra_class)\n",
    "        file_name = 'vgg-'+str(args.depth)\n",
    "#         net.apply(vggnet.conv_init)\n",
    "        net.load_state_dict(torch.load(f\"vgg16_cifar_model_9.pt\"))\n",
    "\n",
    "    elif (args.net_type == 'resnet'):\n",
    "        net = resnet.ResNet(args.depth, num_classes+extra_class)\n",
    "        file_name = 'resnet-'+str(args.depth)\n",
    "#         net.apply(resnet.conv_init)\n",
    "        net.load_state_dict(torch.load(f\"resnet18_cifar_model_9.pt\"))\n",
    "\n",
    "    elif (args.net_type == 'resnet2'):\n",
    "\n",
    "        if args.dataset == 'mnist' or args.dataset == 'fashion':\n",
    "            num_channels = 1\n",
    "        else:\n",
    "            num_channels = 3\n",
    "\n",
    "        if args.depth == 34:\n",
    "            net = resnet2.ResNet34(num_classes=num_classes+extra_class,num_input_channels=num_channels)\n",
    "            file_name = 'resnet2-34'#+str(args.depth)\n",
    "\n",
    "        elif args.depth == 18:\n",
    "            #pdb.set_trace()\n",
    "            net = resnet2.ResNet18(num_classes=num_classes+extra_class,num_input_channels=num_channels)\n",
    "            file_name = 'resnet2-18'#+str(args.depth)\n",
    "\n",
    "        else:\n",
    "            print('Error : Resnet-2 Network depth should either be 18 or 34')\n",
    "            sys.exit(0)\n",
    "\n",
    "        net.apply(resnet2.conv_init)\n",
    "\n",
    "    elif (args.net_type == 'wide-resnet'):\n",
    "        net = wide_resnet.Wide_ResNet(args.depth, args.widen_factor, args.dropout, num_classes+extra_class)\n",
    "        file_name = 'wide-resnet-'+str(args.depth)+'x'+str(args.widen_factor)\n",
    "        net.apply(wide_resnet.conv_init)\n",
    "\n",
    "    else:\n",
    "        print('Error : Network should be either [LeNet / VGGNet / ResNet / Wide_ResNet')\n",
    "        sys.exit(0)\n",
    "\n",
    "    return net, file_name\n",
    "\n",
    "\n",
    "\n",
    "print('\\n[Phase 2] : Model setup')\n",
    "if args.resume:\n",
    "    # Load checkpoint\n",
    "    print('| Resuming from checkpoint...')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
    "    _, file_name = getNetwork(args)\n",
    "    checkpoint = torch.load('./checkpoint/'+args.dataset+os.sep+file_name+'.t7')\n",
    "    net = checkpoint['net']\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "else:\n",
    "    #print('| Building net type [' + args.net_type + ']...')\n",
    "    print('| Building net')\n",
    "    #net, file_name = getNetwork(args)\n",
    "    #net.apply(conv_init)\n",
    "    if args.net_type is None:\n",
    "        print(\"Using Default conv net\")\n",
    "        file_name = 'conv_net'\n",
    "        if args.loss_fn is None: #no abstention. use the actual number of classes\n",
    "            net = cnn.ConvNet(num_classes,args.dropout)\n",
    "        else: #use extra class for abstention \n",
    "            net = cnn.ConvNet(num_classes+1,args.dropout)\n",
    "\n",
    "    else:\n",
    "        print('| Building net type [' + args.net_type + ']...')\n",
    "        net, file_name = getNetwork(args)\n",
    "        #net.apply(conv_init)\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "#set up loss function and CUDA-fy if needed\n",
    "if args.loss_fn is None:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print('Using regular  (non-abstaining) loss function during training')\n",
    "    if use_cuda:\n",
    "        criterion = nn.CrossEntropyLoss().cuda(cuda_device)\n",
    "else:\n",
    "    if args.loss_fn == 'dac_loss':\n",
    "        if args.abst_rate is None:\n",
    "            criterion = loss_fn_dict['dac_loss'](model=net, learn_epochs=learn_epochs, \n",
    "                total_epochs=epochs,  use_cuda=use_cuda, alpha_final=args.alpha_final, \n",
    "                alpha_init_factor=args.alpha_init_factor)\n",
    "        else:\n",
    "            pid_tunings = (args.k_p, args.k_i, args.k_d)\n",
    "            criterion = loss_fn_dict['dac_loss_pid'](model=net, learn_epochs=learn_epochs,\n",
    "                 total_epochs=epochs, use_cuda=use_cuda, cuda_device=cuda_device, abst_rate=args.abst_rate,\n",
    "                 alpha_final=args.alpha_final,alpha_init_factor=args.alpha_init_factor, pid_tunings=pid_tunings)\n",
    "    else:\n",
    "        print(\"Unknown loss function\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    if use_cuda:\n",
    "        criterion = criterion.cuda(cuda_device)\n",
    "\n",
    "\n",
    "#CUDA-fy network\n",
    "#pdb.set_trace()\n",
    "if use_cuda:\n",
    "    if args.data_parallel:\n",
    "        net = torch.nn.DataParallel(net, device_ids=cuda_devices).cuda(cuda_device)\n",
    "    else:\n",
    "        net = net.cuda(cuda_device)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def get_hms(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "\n",
    "    return h, m, s\n",
    "\n",
    "#pdb.set_trace()\n",
    "def train(epoch):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    abstain = 0\n",
    "\n",
    "    if args.dataset == 'mnist':\n",
    "        if int(epoch/epdl) > 5 and  int(epoch/epdl) <= 20:\n",
    "            args.lr = 0.01\n",
    "        if int(epoch/epdl) > 20 and int(epoch/epdl) <=50:\n",
    "            args.lr = 0.001\n",
    "\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=cf.learning_rate(args.lr, epoch), momentum=0.9, \n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, \n",
    "            nesterov=args.nesterov, weight_decay=5e-4)\n",
    "        print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, args.lr))\n",
    "\n",
    "\n",
    "    else: #cifar 10/100/stl-10/tin200/fashion\n",
    "        optimizer = optim.SGD(net.parameters(), lr=cf.learning_rate(args.lr, int(epoch/epdl)),\n",
    "         momentum=0.9, weight_decay=5e-4,nesterov=args.nesterov)\n",
    "        print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, cf.learning_rate(args.lr, int(epoch/epdl))))\n",
    "\n",
    "    #print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, cf.learning_rate(args.lr, epoch)))\n",
    "    #pdb.set_trace()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        #print(type(inputs))\n",
    "        #print(dir(inputs.cuda))\n",
    "        #quit()\n",
    "        if use_cuda:\n",
    "            #pdb.set_trace()\n",
    "            inputs, targets = inputs.cuda(cuda_device), targets.cuda(cuda_device) # GPU settings\n",
    "            #inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)               # Forward Propagation\n",
    "        #pdb.set_trace()\n",
    "        if args.loss_fn is None:\n",
    "            loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            loss = criterion(outputs, targets, epoch)  # Loss\n",
    "\n",
    "        loss.backward()  # Backward Propagation\n",
    "        optimizer.step() # Optimizer update\n",
    "\n",
    "        train_loss += loss.data.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        this_batch_size =targets.size(0) \n",
    "        total += this_batch_size\n",
    "        correct += predicted.eq(targets.data).cpu().sum().data.item()\n",
    "\n",
    "        abstained_now = predicted.eq(abstain_class_id).sum().data.item()\n",
    "        abstain += abstained_now\n",
    "\n",
    "        if total-abstain != 0:\n",
    "            #pdb.set_trace()\n",
    "            abst_acc = 100.*correct/(float(total-abstain))\n",
    "        else:\n",
    "            abst_acc = 1.\n",
    "\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tAbstained %d Abstention rate %.4f Cumulative Abstention Rate: %.4f Loss: %.4f Acc@1: %.3f%% Acc@2: %.3f%%'\n",
    "                %(epoch, num_epochs, batch_idx+1,\n",
    "                    (len(trainset)//batch_size)+1, abstain, float(abstained_now)/this_batch_size, float(abstain)/total, loss.data.item(), 100.*correct/float(total), abst_acc))\n",
    "\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    #if args.loss_fn == 'dac_loss_pid':\n",
    "    #criterion.print_abst_stats(epoch)\n",
    "\n",
    "\n",
    "\n",
    "def save_train_scores(epoch):\n",
    "    #net.eval()\n",
    "\n",
    "    train_softmax_scores = []\n",
    "    total = 0\n",
    "    abstained = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_perf_loader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(cuda_device), targets.cuda(cuda_device) # GPU settings\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)               # Forward Propagation\n",
    "        p_out = F.softmax(outputs,dim=1)\n",
    "        #pdb.set_trace()\n",
    "        total += p_out.size(0)\n",
    "        _,predicted = torch.max(p_out.data,1)\n",
    "        abstained += predicted.eq(abstain_class_id).sum().data.item()\n",
    "        train_softmax_scores.append(p_out.data)\n",
    "\n",
    "    train_scores = torch.cat(train_softmax_scores).cpu().numpy()\n",
    "    print('Saving train softmax scores at  Epoch %d' %(epoch))\n",
    "    #if args.log_file is None:\n",
    "    # if args.expt_name is None:\n",
    "    # fn = 'test'\n",
    "    # else:\n",
    "    # fn = args.expt_name \n",
    "    fn = args.expt_name if args.expt_name else 'test'\n",
    "    np.save(args.output_path+fn+\".train_scores.epoch_\"+str(epoch), train_scores)\n",
    "    print(\"\\n##### Epoch %d Train Abstention Rate at end of epoch %.4f\" \n",
    "            %(epoch, float(abstained)/total))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    abstain = 0\n",
    "\n",
    "    if args.save_val_scores:\n",
    "        val_softmax_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(cuda_device), targets.cuda(cuda_device)\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "            if args.loss_fn is None:\n",
    "                loss = criterion(outputs, targets)\n",
    "            else:\n",
    "                loss = criterion(outputs, targets, epoch)\n",
    "\n",
    "            if args.save_val_scores:\n",
    "                p_out = F.softmax(outputs,dim=1)\n",
    "                val_softmax_scores.append(p_out.data)\n",
    "\n",
    "\n",
    "            test_loss += loss.data.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # if epoch >= args.learn_epochs-1:\n",
    "            # \tpdb.set_trace()\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum().data.item()\n",
    "            abstain += predicted.eq(abstain_class_id).sum().data.item()\n",
    "\n",
    "        if args.save_val_scores:\n",
    "            val_scores = torch.cat(val_softmax_scores).cpu().numpy()\n",
    "\n",
    "            print('Saving softmax scores at Validation Epoch %d' %(epoch))\n",
    "            fn = args.expt_name if args.expt_name else 'test'\n",
    "            #np.save(fn+\".train_scores.epoch_\"+str(epoch), train_scores)\n",
    "\n",
    "            np.save(args.output_path+fn+\".val_scores.epoch_\"+str(epoch), val_scores)\n",
    "\n",
    "        #pdb.set_trace()\n",
    "        acc = 100.*correct/float(total)\n",
    "        if total-abstain != 0:\n",
    "            abst_acc = 100.*correct/(float(total-abstain))\n",
    "        else:\n",
    "            abst_acc = 100.\n",
    "\n",
    "        print(\"\\n| Validation Epoch #%d\\t\\t\\tAbstained: %d Loss: %.4f Acc@1: %.2f%% Acc@2: %.2f%% \" %(epoch, abstain, test_loss/(batch_idx+1), acc,abst_acc))\n",
    "\n",
    "        #return\n",
    "\n",
    "        # Save checkpoint when best model\n",
    "\n",
    "        if acc > best_acc or epoch == save_epoch_model:# or (int(epoch/args.epdl) > 60 and int(epoch/args.epdl) <= 80):\n",
    "\n",
    "            if args.save_best_model:\n",
    "                print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
    "                state = {\n",
    "                        'net':net if use_cuda else net,\n",
    "                        'acc':acc,\n",
    "                        'epoch':epoch,\n",
    "                }\n",
    "                if not os.path.isdir('checkpoint'):\n",
    "                    os.mkdir('checkpoint')\n",
    "                save_point = './checkpoint/'+args.dataset+os.sep\n",
    "                if not os.path.isdir(save_point):\n",
    "                    os.mkdir(save_point)\n",
    "                #torch.save(state, save_point+file_name+'_rand_label_'+str(args.rand_labels)+'_epoch_'+str(epoch)+'_081318.t7')\n",
    "                if args.expt_name == \"\":\n",
    "                    if not args.log_file is None:\n",
    "                        expt_name = os.path.basename(args.log_file).replace(\".log\",\"\")\n",
    "                    else:\n",
    "                        expt_name = 'test' #assuming that if a log file has not been specified this is a test run.\n",
    "                else:\n",
    "                    expt_name = args.expt_name\n",
    "                if args.no_overwrite:\n",
    "                    torch.save(state, save_point+file_name+'_expt_name_'+str(expt_name)+'_epoch_'+str(epoch)+'.t7')\n",
    "                else:\n",
    "                    torch.save(state, save_point+file_name+'_expt_name_'+str(expt_name)+'.t7')\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "\n",
    "\n",
    "\n",
    "print('\\n[Phase 3] : Training model')\n",
    "print('| Training Epochs = ' + str(num_epochs))\n",
    "print('| Initial Learning Rate = ' + str(args.lr))\n",
    "sys.stdout.flush()\n",
    "\n",
    "#print('| Optimizer = ' + str(optim_type))\n",
    "\n",
    "elapsed_time = 0\n",
    "for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train(epoch)\n",
    "    if args.save_train_scores:\n",
    "        save_train_scores(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    elapsed_time += epoch_time\n",
    "    print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print('\\n[Phase 4] : Testing model')\n",
    "print('* Test results : Acc@1 = %.2f%%' %(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c7d9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net,'DAC_vgg16_cifar_9.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4b9d6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "def get_SVHN():\n",
    "    dataset = datasets.SVHN\n",
    "    num_classes = 10\n",
    "    input_size = 32\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32,padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    train_dataset = dataset(root='./data', split='train', transform=transform_train,\n",
    "                                     target_transform=None, download=True)\n",
    "    test_dataset = dataset(root='./data', split='test', transform=transform_test,\n",
    "                                   target_transform=None, download=True)\n",
    "    return train_dataset,test_dataset,num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f088f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "def get_CIFAR10(root=\"./\"):\n",
    "    input_size = 32\n",
    "    num_classes = 10\n",
    "    normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    \n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root + \"data/CIFAR10\", train=True, transform=train_transform, download=True\n",
    "    )\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root + \"data/CIFAR10\", train=False, transform=test_transform, download=True\n",
    "    )\n",
    "\n",
    "    return train_dataset, test_dataset, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcdab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
    "        \n",
    "        outputs = net(inputs)               # Forward Propagation\n",
    "        p_out = F.softmax(outputs,dim=1)\n",
    "        output.extend(p_out)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
