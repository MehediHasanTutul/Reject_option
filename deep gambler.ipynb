{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e112ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "# import models.cifar as models\n",
    "import dataset_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5412d194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    " dataset = datasets.CIFAR10\n",
    "num_classes = 10\n",
    "input_size = 32\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "trainset = dataset(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = dataset(root='./data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb8abf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    " dataset = datasets.SVHN\n",
    "num_classes = 10\n",
    "input_size = 32\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32,padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "train_dataset = dataset(root='./data', split='train', transform=transform_train,\n",
    "                                 target_transform=None, download=True)\n",
    "test_dataset = dataset(root='./data', split='test', transform=transform_test,\n",
    "                               target_transform=None, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3920e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_size = 64\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(64, padding=6),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "# use the \"ImageFolder\" datasets\n",
    "assert os.path.exists(\"./data/cats_dogs/train\") and os.path.exists(\"./data/cats_dogs/test\"), \"Please download and put the 'cats vs dogs' dataset to paths 'data/cats_dogs/train' and 'cats_dogs/test'\"\n",
    "trainset =  datasets.ImageFolder('./data/cats_dogs/train')\n",
    "testset =  datasets.ImageFolder('./data/cats_dogs/test')\n",
    "# resizing the images to 64 and center crop them, so that they become 64x64 squares\n",
    "trainset = dataset_utils.resized_dataset(trainset, transform_train, resize=64)\n",
    "testset = dataset_utils.resized_dataset(testset, transform_test, resize=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "50d3d50e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13540/2953641344.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainset' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "#         self.resnet = models.resnet18(pretrained=False, num_classes=10)\n",
    "\n",
    "#         self.resnet.conv1 = torch.nn.Conv2d(\n",
    "#             3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "#         )\n",
    "#         self.resnet.maxpool = torch.nn.Identity()\n",
    "        \n",
    "        self.resnet = models.vgg16(pretrained=False, num_classes=10+1)\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "#         x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d839c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda=True\n",
    "# Model\n",
    "print(\"==> creating model '{}'\".format('vgg16'))\n",
    "# model = models.__dict__['vgg16'](num_classes=num_classes+1, input_size = input_size)\n",
    "model=Model()\n",
    "# 'alexnet': <function models.cifar.alexnet.alexnet(**kwargs)>,\n",
    "#  'vgg': <module 'models.cifar.vgg' from 'C:\\\\Users\\\\Student\\\\models\\\\cifar\\\\vgg.py'>,\n",
    "#  'VGG': models.cifar.vgg.VGG,\n",
    "#  'vgg16': <function models.cifar.vgg.vgg16(**kwargs)>,\n",
    "#  'vgg16_bn': <function models.cifar.vgg.vgg16_bn(**kwargs)>,\n",
    "#  'resnet': <function models.cifar.resnet.resnet(**kwargs)>,\n",
    "#  'resnext': <function models.cifar.resnext.resnext(**kwargs)>,\n",
    "#  'wrn': <function models.cifar.wrn.wrn(**kwargs)>,\n",
    "#  'preresnet': <function models.cifar.preresnet.preresnet(**kwargs)>,\n",
    "#  'densenet': <function models.cifar.densenet.densenet(**kwargs)>}\n",
    "if use_cuda: model = torch.nn.DataParallel(model.cuda())\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "pretrain=10\n",
    "\n",
    "if pretrain: criterion = nn.CrossEntropyLoss() \n",
    "# the conventional loss is replaced by the gambler's loss in train() and test() explicitly except for pretraining\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd50893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01362eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.5\n",
    "schedule=[20,40,75]\n",
    "state={}\n",
    "state['lr']=0.001\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in schedule:\n",
    "        state['lr'] *= gamma\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ed7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\n",
    "       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c78008",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward=2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    \n",
    "    for batch_idx,  (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "#         inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        if epoch >= pretrain:\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            outputs, reservation = outputs[:,:-1], outputs[:,-1]\n",
    "            gain = torch.gather(outputs, dim=1, index=targets.unsqueeze(1)).squeeze()\n",
    "            doubling_rate = (gain.add(reservation.div(reward))).log()\n",
    "\n",
    "            loss = -doubling_rate.mean()\n",
    "        else:\n",
    "            loss = criterion(outputs[:,:-1], targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        progress  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    )\n",
    "    print(progress)\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(testloader, model, criterion, epoch, use_cuda, evaluation = False):\n",
    "    global best_acc\n",
    "    expected_coverage=[100.,99.,98.,97.,95.,]\n",
    "    # whether to evaluate uncertainty, or confidence\n",
    "    if evaluation:\n",
    "        evaluate(testloader, model, use_cuda)\n",
    "        return\n",
    "\n",
    "    # switch to test mode\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    reservation=[]\n",
    "    abstention_results = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            if epoch >= pretrain:\n",
    "                outputs = F.softmax(outputs, dim=1)\n",
    "                outputs, reservation = outputs[:,:-1], outputs[:,-1]\n",
    "                predictions = outputs.data.max(1)[1]\n",
    "                # analyze the accuracy at different abstention level\n",
    "                abstention_results.extend(zip(list( reservation.cpu() ),list( predictions.eq(targets.data).cpu() )))\n",
    "                \n",
    "                # calculate loss\n",
    "                gain = torch.gather(outputs, dim=1, index=targets.unsqueeze(1)).squeeze()\n",
    "                doubling_rate = (gain.add(reservation.div(reward))).log()\n",
    "                loss = -doubling_rate.mean()\n",
    "            else:\n",
    "                loss = criterion(outputs[:,:-1], targets)\n",
    "#             return outputs, reservation, abstention_results\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        res=abstention_results\n",
    "        \n",
    "        # plot progress\n",
    "        suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(testloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    )\n",
    "    print(suffix)\n",
    "    if epoch >= pretrain:\n",
    "        # sort the abstention results according to their reservations, from high to low\n",
    "        abstention_results.sort(key = lambda x: x[0])\n",
    "        # get the \"correct or not\" list for the sorted results\n",
    "        sorted_correct = list(map(lambda x: int(x[1]), abstention_results))\n",
    "        size = len(sorted_correct)\n",
    "        print('accracy of coverage ',end='')\n",
    "        for coverage in expected_coverage:\n",
    "            covered_correct = sorted_correct[:round(size/100*coverage)]\n",
    "            print('{:.0f}: {:.3f}, '.format(coverage, sum(covered_correct)/len(covered_correct)*100.), end='')\n",
    "        print('')\n",
    "    return (losses.avg, top1.avg,res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab6b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, reservation, abstention_resultsreservation=test_loss, test_acc, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "300259a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10%6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate=True\n",
    "if evaluate:\n",
    "    print('\\nEvaluation only')\n",
    "    test(testloader, model, criterion, 0, use_cuda, evaluation = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ad2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "for epoch in range(0, epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "#         print('\\n'+save_path)\n",
    "#         print('Epoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "\n",
    "        train_loss, train_acc = train(trainloader, model, criterion, optimizer, epoch, use_cuda)\n",
    "        test_loss, test_acc, res = test(testloader, model, criterion, epoch, use_cuda)\n",
    "        # save the model\n",
    "#         filepath = os.path.join(save_path, \"{:d}\".format(epoch+1) + \".pth\")\n",
    "#         torch.save(model, filepath)\n",
    "#         # delete the last saved model if exist\n",
    "#         last_path = os.path.join(save_path, \"{:d}\".format(epoch) + \".pth\")\n",
    "#         if os.path.isfile(last_path): os.remove(last_path)\n",
    "#         # append logger file\n",
    "#         logger.append([epoch+1, state['lr'], train_loss, test_loss, 100-train_acc, 100-test_acc])\n",
    "\n",
    "# filepath = os.path.join(save_path, \"{:d}\".format(args.epochs) + \".pth\")\n",
    "# torch.save(model, filepath)\n",
    "# last_path = os.path.join(save_path, \"{:d}\".format(args.epochs-1) + \".pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef6efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_CIFAR10(root=\"./\"):\n",
    "    input_size = 32\n",
    "    num_classes = 10\n",
    "    normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    \n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root + \"data/CIFAR10\", train=True, transform=train_transform, download=True\n",
    "    )\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root + \"data/CIFAR10\", train=False, transform=test_transform, download=True\n",
    "    )\n",
    "\n",
    "    return input_size, num_classes, train_dataset, test_dataset\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch,alpha=1.0):\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "#     const=1-epoch/50\n",
    "    total_loss = []\n",
    "    \n",
    "    for data, target in train_loader:  #tqdm()\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(data)\n",
    "        if epoch >= pretrain:\n",
    "            outputs = F.softmax(prediction, dim=1)\n",
    "            outputs, reservation = outputs[:,:-1], outputs[:,-1]\n",
    "            gain = torch.gather(outputs, dim=1, index=target.unsqueeze(1)).squeeze()\n",
    "            doubling_rate = (gain.add(reservation.div(reward))).log()\n",
    "\n",
    "            loss = -doubling_rate.mean()\n",
    "        else:\n",
    "            \n",
    "#             loss = F.cross_entropy(prediction[:,:-1], target) #cross_entropy  nll_loss\n",
    "            \n",
    "            loss = NotWrongLoss(prediction,target,alpha=alpha)\n",
    "#             print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    avg_loss = sum(total_loss) / len(total_loss)\n",
    "    print(f\"Epoch: {epoch}:\")\n",
    "    print(f\"Train Set: Average Loss: {avg_loss:.2f}\")\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    abstention_results=[]\n",
    "    for data, target in test_loader:\n",
    "        with torch.no_grad():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "            prediction = model(data)\n",
    "            if epoch >= pretrain:\n",
    "                outputs = F.softmax(prediction, dim=1)\n",
    "                outputs, reservation = outputs[:,:-1], outputs[:,-1]\n",
    "                predictions = outputs.data.max(1)[1]\n",
    "                # analyze the accuracy at different abstention level\n",
    "                abstention_results.extend(zip(list( reservation.cpu() ),list( predictions.eq(target.data).cpu() )))\n",
    "                \n",
    "                # calculate loss\n",
    "                gain = torch.gather(outputs, dim=1, index=target.unsqueeze(1)).squeeze()\n",
    "                doubling_rate = (gain.add(reservation.div(reward))).log()\n",
    "                loss = -doubling_rate.mean()\n",
    "            else:\n",
    "                \n",
    "                \n",
    "                loss += F.nll_loss(prediction[:,:-1], target, reduction=\"sum\")\n",
    "\n",
    "            prediction = prediction.max(1)[1]\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    loss /= len(test_loader.dataset)\n",
    "\n",
    "    percentage_correct = 100.0 * correct / len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\".format(\n",
    "            loss, correct, len(test_loader.dataset), percentage_correct\n",
    "        )\n",
    "    )\n",
    "    if epoch >= pretrain:\n",
    "        # sort the abstention results according to their reservations, from high to low\n",
    "        abstention_results.sort(key = lambda x: x[0])\n",
    "        # get the \"correct or not\" list for the sorted results\n",
    "        sorted_correct = list(map(lambda x: int(x[1]), abstention_results))\n",
    "        size = len(sorted_correct)\n",
    "        print('accracy of coverage ',end='')\n",
    "        expected_coverage=[100.,98.,95.]\n",
    "        for coverage in expected_coverage:\n",
    "            covered_correct = sorted_correct[:round(size/100*coverage)]\n",
    "            print(covered_correct)\n",
    "            print('{:.0f}: {:.3f}, '.format(coverage, sum(covered_correct)/len(covered_correct)*100.), end='')\n",
    "        print('')\n",
    "    \n",
    "    return loss, percentage_correct\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "#         self.resnet = models.resnet18(pretrained=False, num_classes=10)\n",
    "\n",
    "#         self.resnet.conv1 = torch.nn.Conv2d(\n",
    "#             3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "#         )\n",
    "#         self.resnet.maxpool = torch.nn.Identity()\n",
    "        \n",
    "        self.resnet = models.vgg16(pretrained=False, num_classes=10+1)\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1f6f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Not wrong loss\n",
    "def NotWrongLoss(outputs,label,alpha=1.5):\n",
    "\n",
    "    outputs=F.softmax(outputs, dim=1)\n",
    "    # calculate loss\n",
    "    gain = torch.gather(outputs, dim=1, index=label.unsqueeze(1)).squeeze()+outputs[:,-1]+1e-8\n",
    "    NW_Loss = -gain.log()-alpha*torch.log(outputs[:,:-1].sum(1)+1e-8)\n",
    "    \n",
    "    return NW_Loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c3700b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)#(args.seed)\n",
    "\n",
    "# input_size, num_classes, train_dataset, test_dataset = get_CIFAR10()\n",
    "\n",
    "kwargs = {\"num_workers\": 2, \"pin_memory\": True}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True, **kwargs\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, **kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e20f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -7.9929, Accuracy: 25021/26032 (96.12%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -7.9701, Accuracy: 25020/26032 (96.11%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1242, Accuracy: 25030/26032 (96.15%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.0836, Accuracy: 25040/26032 (96.19%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1436, Accuracy: 25038/26032 (96.18%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1308, Accuracy: 25048/26032 (96.22%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1218, Accuracy: 25011/26032 (96.08%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1999, Accuracy: 25049/26032 (96.22%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1487, Accuracy: 25050/26032 (96.23%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1612, Accuracy: 25031/26032 (96.15%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -7.9181, Accuracy: 25037/26032 (96.18%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -7.9607, Accuracy: 25041/26032 (96.19%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -7.9245, Accuracy: 25029/26032 (96.15%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0496, Accuracy: 25026/26032 (96.14%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0887, Accuracy: 25030/26032 (96.15%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0205, Accuracy: 25026/26032 (96.14%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0296, Accuracy: 25023/26032 (96.12%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1152, Accuracy: 24967/26032 (95.91%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.0131, Accuracy: 25045/26032 (96.21%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1850, Accuracy: 24972/26032 (95.93%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2272, Accuracy: 25041/26032 (96.19%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2159, Accuracy: 25013/26032 (96.09%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2876, Accuracy: 25009/26032 (96.07%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3124, Accuracy: 25015/26032 (96.09%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2713, Accuracy: 25026/26032 (96.14%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3069, Accuracy: 24980/26032 (95.96%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3278, Accuracy: 24992/26032 (96.00%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3664, Accuracy: 24996/26032 (96.02%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4240, Accuracy: 24979/26032 (95.95%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4514, Accuracy: 24988/26032 (95.99%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0486, Accuracy: 25107/26032 (96.45%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0042, Accuracy: 25080/26032 (96.34%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.0535, Accuracy: 25103/26032 (96.43%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1699, Accuracy: 25118/26032 (96.49%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1646, Accuracy: 25117/26032 (96.49%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2198, Accuracy: 25106/26032 (96.44%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1679, Accuracy: 25087/26032 (96.37%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2362, Accuracy: 25086/26032 (96.37%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2060, Accuracy: 25069/26032 (96.30%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2653, Accuracy: 25116/26032 (96.48%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1730, Accuracy: 25052/26032 (96.24%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1608, Accuracy: 25057/26032 (96.25%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2587, Accuracy: 25046/26032 (96.21%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1910, Accuracy: 25033/26032 (96.16%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2913, Accuracy: 25021/26032 (96.12%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2943, Accuracy: 25063/26032 (96.28%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3227, Accuracy: 25061/26032 (96.27%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3895, Accuracy: 25048/26032 (96.22%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3809, Accuracy: 25037/26032 (96.18%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4312, Accuracy: 25070/26032 (96.30%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2843, Accuracy: 25118/26032 (96.49%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3309, Accuracy: 25117/26032 (96.49%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3440, Accuracy: 25103/26032 (96.43%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3870, Accuracy: 25089/26032 (96.38%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4110, Accuracy: 25084/26032 (96.36%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3844, Accuracy: 25094/26032 (96.40%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4586, Accuracy: 25093/26032 (96.39%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4698, Accuracy: 25099/26032 (96.42%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4436, Accuracy: 25099/26032 (96.42%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4596, Accuracy: 25101/26032 (96.42%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1423, Accuracy: 25098/26032 (96.41%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1857, Accuracy: 25080/26032 (96.34%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2245, Accuracy: 25072/26032 (96.31%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2207, Accuracy: 25072/26032 (96.31%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2468, Accuracy: 25084/26032 (96.36%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2978, Accuracy: 25104/26032 (96.44%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3032, Accuracy: 25072/26032 (96.31%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3402, Accuracy: 25090/26032 (96.38%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3412, Accuracy: 25093/26032 (96.39%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3741, Accuracy: 25049/26032 (96.22%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3186, Accuracy: 25061/26032 (96.27%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3456, Accuracy: 25096/26032 (96.40%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3776, Accuracy: 25068/26032 (96.30%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3961, Accuracy: 25055/26032 (96.25%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4567, Accuracy: 25090/26032 (96.38%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4787, Accuracy: 25080/26032 (96.34%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4999, Accuracy: 25066/26032 (96.29%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4918, Accuracy: 25064/26032 (96.28%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5995, Accuracy: 25045/26032 (96.21%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5383, Accuracy: 25050/26032 (96.23%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1165, Accuracy: 25038/26032 (96.18%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1824, Accuracy: 25060/26032 (96.27%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2280, Accuracy: 25049/26032 (96.22%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2245, Accuracy: 25030/26032 (96.15%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2769, Accuracy: 25017/26032 (96.10%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3190, Accuracy: 25018/26032 (96.10%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3134, Accuracy: 25017/26032 (96.10%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3024, Accuracy: 25038/26032 (96.18%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3050, Accuracy: 25025/26032 (96.13%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4476, Accuracy: 25045/26032 (96.21%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4290, Accuracy: 25073/26032 (96.32%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4220, Accuracy: 25050/26032 (96.23%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4357, Accuracy: 25042/26032 (96.20%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4457, Accuracy: 25047/26032 (96.22%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4878, Accuracy: 25061/26032 (96.27%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5885, Accuracy: 25055/26032 (96.25%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5352, Accuracy: 25015/26032 (96.09%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5654, Accuracy: 25066/26032 (96.29%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5467, Accuracy: 25063/26032 (96.28%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5775, Accuracy: 25072/26032 (96.31%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -7.9809, Accuracy: 25050/26032 (96.23%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.0601, Accuracy: 25053/26032 (96.24%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1061, Accuracy: 25025/26032 (96.13%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.0909, Accuracy: 25001/26032 (96.04%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1567, Accuracy: 25025/26032 (96.13%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2091, Accuracy: 25040/26032 (96.19%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1594, Accuracy: 25029/26032 (96.15%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2289, Accuracy: 25059/26032 (96.26%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2751, Accuracy: 25062/26032 (96.27%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2634, Accuracy: 25014/26032 (96.09%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -7.8393, Accuracy: 24999/26032 (96.03%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -7.9954, Accuracy: 25001/26032 (96.04%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -7.9440, Accuracy: 25014/26032 (96.09%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0608, Accuracy: 25029/26032 (96.15%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0462, Accuracy: 25002/26032 (96.04%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0305, Accuracy: 24975/26032 (95.94%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1604, Accuracy: 25010/26032 (96.07%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2013, Accuracy: 25009/26032 (96.07%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1249, Accuracy: 24972/26032 (95.93%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2401, Accuracy: 24996/26032 (96.02%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2859, Accuracy: 25023/26032 (96.12%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2356, Accuracy: 25018/26032 (96.10%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3419, Accuracy: 25000/26032 (96.04%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3591, Accuracy: 25000/26032 (96.04%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2994, Accuracy: 24997/26032 (96.02%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3860, Accuracy: 25004/26032 (96.05%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4633, Accuracy: 25020/26032 (96.11%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4787, Accuracy: 25042/26032 (96.20%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4471, Accuracy: 24996/26032 (96.02%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5169, Accuracy: 24999/26032 (96.03%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0717, Accuracy: 25089/26032 (96.38%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1211, Accuracy: 25091/26032 (96.39%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.0455, Accuracy: 25095/26032 (96.40%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1229, Accuracy: 25080/26032 (96.34%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1096, Accuracy: 25081/26032 (96.35%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1854, Accuracy: 25082/26032 (96.35%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1265, Accuracy: 25084/26032 (96.36%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2397, Accuracy: 25064/26032 (96.28%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2720, Accuracy: 25075/26032 (96.32%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3304, Accuracy: 25050/26032 (96.23%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1391, Accuracy: 25061/26032 (96.27%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2452, Accuracy: 25083/26032 (96.35%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2324, Accuracy: 25041/26032 (96.19%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3030, Accuracy: 25026/26032 (96.14%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3126, Accuracy: 25064/26032 (96.28%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3066, Accuracy: 25050/26032 (96.23%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4439, Accuracy: 25061/26032 (96.27%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4112, Accuracy: 25073/26032 (96.32%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4782, Accuracy: 25064/26032 (96.28%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3939, Accuracy: 25027/26032 (96.14%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3964, Accuracy: 25122/26032 (96.50%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3716, Accuracy: 25115/26032 (96.48%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3817, Accuracy: 25104/26032 (96.44%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4138, Accuracy: 25107/26032 (96.45%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4506, Accuracy: 25105/26032 (96.44%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4503, Accuracy: 25114/26032 (96.47%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4700, Accuracy: 25121/26032 (96.50%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4988, Accuracy: 25106/26032 (96.44%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5206, Accuracy: 25111/26032 (96.46%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5487, Accuracy: 25112/26032 (96.47%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1809, Accuracy: 25072/26032 (96.31%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1498, Accuracy: 25089/26032 (96.38%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2253, Accuracy: 25037/26032 (96.18%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3301, Accuracy: 25088/26032 (96.37%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2910, Accuracy: 25084/26032 (96.36%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2892, Accuracy: 25096/26032 (96.40%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2927, Accuracy: 25074/26032 (96.32%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3723, Accuracy: 25090/26032 (96.38%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4276, Accuracy: 25094/26032 (96.40%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4441, Accuracy: 25089/26032 (96.38%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2758, Accuracy: 25062/26032 (96.27%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3881, Accuracy: 25089/26032 (96.38%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4567, Accuracy: 25090/26032 (96.38%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4653, Accuracy: 25075/26032 (96.32%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3947, Accuracy: 25056/26032 (96.25%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5132, Accuracy: 25073/26032 (96.32%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4949, Accuracy: 25083/26032 (96.35%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5708, Accuracy: 25087/26032 (96.37%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6074, Accuracy: 25080/26032 (96.34%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6118, Accuracy: 25050/26032 (96.23%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1733, Accuracy: 25029/26032 (96.15%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1928, Accuracy: 25039/26032 (96.19%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2365, Accuracy: 25039/26032 (96.19%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2745, Accuracy: 25061/26032 (96.27%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3161, Accuracy: 25065/26032 (96.29%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2954, Accuracy: 25011/26032 (96.08%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3279, Accuracy: 25052/26032 (96.24%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3641, Accuracy: 25047/26032 (96.22%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3228, Accuracy: 25048/26032 (96.22%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4767, Accuracy: 25072/26032 (96.31%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3124, Accuracy: 25041/26032 (96.19%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4358, Accuracy: 25095/26032 (96.40%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5268, Accuracy: 25088/26032 (96.37%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5225, Accuracy: 25068/26032 (96.30%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5592, Accuracy: 25088/26032 (96.37%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5721, Accuracy: 25063/26032 (96.28%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5360, Accuracy: 25064/26032 (96.28%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6348, Accuracy: 25086/26032 (96.37%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6359, Accuracy: 25083/26032 (96.35%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6426, Accuracy: 25057/26032 (96.25%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.0378, Accuracy: 25016/26032 (96.10%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1106, Accuracy: 25029/26032 (96.15%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.0874, Accuracy: 25010/26032 (96.07%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1729, Accuracy: 25043/26032 (96.20%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2268, Accuracy: 25049/26032 (96.22%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2554, Accuracy: 25026/26032 (96.14%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2609, Accuracy: 25051/26032 (96.23%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2891, Accuracy: 25029/26032 (96.15%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3482, Accuracy: 25045/26032 (96.21%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3789, Accuracy: 25035/26032 (96.17%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -7.9688, Accuracy: 25034/26032 (96.17%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0166, Accuracy: 25012/26032 (96.08%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1086, Accuracy: 25037/26032 (96.18%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1157, Accuracy: 25026/26032 (96.14%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0883, Accuracy: 25019/26032 (96.11%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1143, Accuracy: 25016/26032 (96.10%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1693, Accuracy: 25025/26032 (96.13%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1819, Accuracy: 24995/26032 (96.02%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1459, Accuracy: 25005/26032 (96.05%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2685, Accuracy: 25035/26032 (96.17%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2644, Accuracy: 25046/26032 (96.21%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3386, Accuracy: 25050/26032 (96.23%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3441, Accuracy: 25047/26032 (96.22%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3922, Accuracy: 25010/26032 (96.07%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4500, Accuracy: 25023/26032 (96.12%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4466, Accuracy: 25032/26032 (96.16%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4593, Accuracy: 25012/26032 (96.08%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5401, Accuracy: 25052/26032 (96.24%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4107, Accuracy: 24977/26032 (95.95%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5845, Accuracy: 25030/26032 (96.15%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0495, Accuracy: 25069/26032 (96.30%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1053, Accuracy: 25090/26032 (96.38%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1203, Accuracy: 25096/26032 (96.40%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1974, Accuracy: 25045/26032 (96.21%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2319, Accuracy: 25079/26032 (96.34%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2611, Accuracy: 25059/26032 (96.26%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2333, Accuracy: 25047/26032 (96.22%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3229, Accuracy: 25065/26032 (96.29%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3025, Accuracy: 25077/26032 (96.33%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4189, Accuracy: 25078/26032 (96.34%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2101, Accuracy: 25064/26032 (96.28%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1954, Accuracy: 25050/26032 (96.23%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2872, Accuracy: 25087/26032 (96.37%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3321, Accuracy: 25042/26032 (96.20%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3622, Accuracy: 25085/26032 (96.36%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3886, Accuracy: 25050/26032 (96.23%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4954, Accuracy: 25090/26032 (96.38%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5532, Accuracy: 25076/26032 (96.33%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5443, Accuracy: 25041/26032 (96.19%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5307, Accuracy: 25048/26032 (96.22%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3653, Accuracy: 25116/26032 (96.48%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3566, Accuracy: 25101/26032 (96.42%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4345, Accuracy: 25128/26032 (96.53%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4455, Accuracy: 25093/26032 (96.39%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4243, Accuracy: 25117/26032 (96.49%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5240, Accuracy: 25123/26032 (96.51%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5157, Accuracy: 25118/26032 (96.49%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5348, Accuracy: 25116/26032 (96.48%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5529, Accuracy: 25098/26032 (96.41%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6188, Accuracy: 25114/26032 (96.47%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2028, Accuracy: 25117/26032 (96.49%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2363, Accuracy: 25113/26032 (96.47%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3078, Accuracy: 25089/26032 (96.38%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2882, Accuracy: 25096/26032 (96.40%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3871, Accuracy: 25069/26032 (96.30%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3472, Accuracy: 25085/26032 (96.36%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4040, Accuracy: 25094/26032 (96.40%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4835, Accuracy: 25097/26032 (96.41%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3833, Accuracy: 25064/26032 (96.28%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4680, Accuracy: 25082/26032 (96.35%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3440, Accuracy: 25084/26032 (96.36%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3678, Accuracy: 25074/26032 (96.32%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4344, Accuracy: 25086/26032 (96.37%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4818, Accuracy: 25061/26032 (96.27%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4669, Accuracy: 25072/26032 (96.31%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4925, Accuracy: 25045/26032 (96.21%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5492, Accuracy: 25066/26032 (96.29%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5397, Accuracy: 25072/26032 (96.31%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6187, Accuracy: 25068/26032 (96.30%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6607, Accuracy: 25094/26032 (96.40%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1945, Accuracy: 25034/26032 (96.17%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1986, Accuracy: 25048/26032 (96.22%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2609, Accuracy: 25045/26032 (96.21%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2704, Accuracy: 25009/26032 (96.07%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3147, Accuracy: 25034/26032 (96.17%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2572, Accuracy: 25033/26032 (96.16%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3084, Accuracy: 25015/26032 (96.09%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4038, Accuracy: 25025/26032 (96.13%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4116, Accuracy: 25032/26032 (96.16%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4799, Accuracy: 25016/26032 (96.10%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4259, Accuracy: 25070/26032 (96.30%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4206, Accuracy: 25057/26032 (96.25%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4662, Accuracy: 25065/26032 (96.29%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5063, Accuracy: 25057/26032 (96.25%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5330, Accuracy: 25035/26032 (96.17%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5439, Accuracy: 25039/26032 (96.19%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5751, Accuracy: 25069/26032 (96.30%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5906, Accuracy: 25057/26032 (96.25%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5177, Accuracy: 25009/26032 (96.07%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6342, Accuracy: 25062/26032 (96.27%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.0614, Accuracy: 25032/26032 (96.16%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1010, Accuracy: 25036/26032 (96.17%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1362, Accuracy: 25039/26032 (96.19%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2119, Accuracy: 25054/26032 (96.24%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2387, Accuracy: 25014/26032 (96.09%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2149, Accuracy: 25019/26032 (96.11%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2686, Accuracy: 25029/26032 (96.15%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3211, Accuracy: 25032/26032 (96.16%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2840, Accuracy: 25020/26032 (96.11%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3957, Accuracy: 25056/26032 (96.25%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -7.9784, Accuracy: 25044/26032 (96.20%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -7.9696, Accuracy: 25017/26032 (96.10%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1407, Accuracy: 25029/26032 (96.15%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1198, Accuracy: 25001/26032 (96.04%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.2252, Accuracy: 25013/26032 (96.09%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1532, Accuracy: 25017/26032 (96.10%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.2567, Accuracy: 24983/26032 (95.97%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1958, Accuracy: 24987/26032 (95.99%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.2583, Accuracy: 25030/26032 (96.15%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.3267, Accuracy: 25013/26032 (96.09%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2187, Accuracy: 24985/26032 (95.98%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3033, Accuracy: 25005/26032 (96.05%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3844, Accuracy: 25030/26032 (96.15%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3754, Accuracy: 25027/26032 (96.14%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3663, Accuracy: 24979/26032 (95.95%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4832, Accuracy: 25019/26032 (96.11%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5505, Accuracy: 25022/26032 (96.12%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5378, Accuracy: 25031/26032 (96.15%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5523, Accuracy: 25020/26032 (96.11%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5868, Accuracy: 25036/26032 (96.17%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1279, Accuracy: 25084/26032 (96.36%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0883, Accuracy: 25060/26032 (96.27%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1865, Accuracy: 25072/26032 (96.31%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2662, Accuracy: 25087/26032 (96.37%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2395, Accuracy: 25117/26032 (96.49%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3209, Accuracy: 25122/26032 (96.50%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3231, Accuracy: 25098/26032 (96.41%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3440, Accuracy: 25061/26032 (96.27%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3272, Accuracy: 25083/26032 (96.35%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3678, Accuracy: 25086/26032 (96.37%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2307, Accuracy: 25078/26032 (96.34%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2008, Accuracy: 25056/26032 (96.25%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2954, Accuracy: 25044/26032 (96.20%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3038, Accuracy: 25036/26032 (96.17%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3606, Accuracy: 24993/26032 (96.01%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4028, Accuracy: 25065/26032 (96.29%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4356, Accuracy: 25071/26032 (96.31%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4953, Accuracy: 25021/26032 (96.12%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5554, Accuracy: 25064/26032 (96.28%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5907, Accuracy: 25059/26032 (96.26%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3230, Accuracy: 25113/26032 (96.47%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3927, Accuracy: 25115/26032 (96.48%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4598, Accuracy: 25103/26032 (96.43%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4249, Accuracy: 25101/26032 (96.42%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5194, Accuracy: 25111/26032 (96.46%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6050, Accuracy: 25120/26032 (96.50%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5537, Accuracy: 25087/26032 (96.37%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6128, Accuracy: 25108/26032 (96.45%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6407, Accuracy: 25083/26032 (96.35%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6233, Accuracy: 25083/26032 (96.35%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2283, Accuracy: 25107/26032 (96.45%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2547, Accuracy: 25089/26032 (96.38%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2738, Accuracy: 25081/26032 (96.35%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3045, Accuracy: 25068/26032 (96.30%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3218, Accuracy: 25098/26032 (96.41%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3818, Accuracy: 25101/26032 (96.42%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3694, Accuracy: 25083/26032 (96.35%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4636, Accuracy: 25093/26032 (96.39%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4942, Accuracy: 25110/26032 (96.46%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5260, Accuracy: 25114/26032 (96.47%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2871, Accuracy: 25056/26032 (96.25%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3510, Accuracy: 25086/26032 (96.37%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3852, Accuracy: 25080/26032 (96.34%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4419, Accuracy: 25063/26032 (96.28%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4373, Accuracy: 25075/26032 (96.32%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5167, Accuracy: 25072/26032 (96.31%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5691, Accuracy: 25081/26032 (96.35%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6003, Accuracy: 25072/26032 (96.31%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6426, Accuracy: 25061/26032 (96.27%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7327, Accuracy: 25118/26032 (96.49%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1262, Accuracy: 25008/26032 (96.07%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2682, Accuracy: 25061/26032 (96.27%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2657, Accuracy: 25034/26032 (96.17%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2882, Accuracy: 25065/26032 (96.29%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3756, Accuracy: 25056/26032 (96.25%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3661, Accuracy: 25063/26032 (96.28%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4825, Accuracy: 25070/26032 (96.30%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4159, Accuracy: 25015/26032 (96.09%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4854, Accuracy: 25038/26032 (96.18%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5148, Accuracy: 25021/26032 (96.12%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4350, Accuracy: 25078/26032 (96.34%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4575, Accuracy: 25060/26032 (96.27%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5168, Accuracy: 25072/26032 (96.31%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5335, Accuracy: 25054/26032 (96.24%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6046, Accuracy: 25056/26032 (96.25%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6448, Accuracy: 25071/26032 (96.31%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6349, Accuracy: 25052/26032 (96.24%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6374, Accuracy: 25036/26032 (96.17%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7082, Accuracy: 25042/26032 (96.20%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7056, Accuracy: 25054/26032 (96.24%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.0747, Accuracy: 25047/26032 (96.22%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1955, Accuracy: 25035/26032 (96.17%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1837, Accuracy: 25018/26032 (96.10%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3041, Accuracy: 25040/26032 (96.19%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2528, Accuracy: 25039/26032 (96.19%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2859, Accuracy: 24997/26032 (96.02%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4323, Accuracy: 25034/26032 (96.17%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4885, Accuracy: 25034/26032 (96.17%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4489, Accuracy: 25016/26032 (96.10%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4928, Accuracy: 25011/26032 (96.08%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0256, Accuracy: 25019/26032 (96.11%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1628, Accuracy: 25043/26032 (96.20%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1402, Accuracy: 25023/26032 (96.12%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1772, Accuracy: 25051/26032 (96.23%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.2799, Accuracy: 25022/26032 (96.12%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.3047, Accuracy: 24971/26032 (95.92%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.3277, Accuracy: 24984/26032 (95.97%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.4864, Accuracy: 24997/26032 (96.02%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.4016, Accuracy: 24961/26032 (95.89%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.4333, Accuracy: 25028/26032 (96.14%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3410, Accuracy: 25020/26032 (96.11%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3972, Accuracy: 25020/26032 (96.11%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4080, Accuracy: 25019/26032 (96.11%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4902, Accuracy: 25035/26032 (96.17%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5367, Accuracy: 25035/26032 (96.17%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5330, Accuracy: 24983/26032 (95.97%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5561, Accuracy: 25029/26032 (96.15%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6009, Accuracy: 25005/26032 (96.05%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6797, Accuracy: 25029/26032 (96.15%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6311, Accuracy: 24987/26032 (95.99%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.0739, Accuracy: 25104/26032 (96.44%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.1687, Accuracy: 25089/26032 (96.38%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.2351, Accuracy: 25096/26032 (96.40%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.2705, Accuracy: 25097/26032 (96.41%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3499, Accuracy: 25078/26032 (96.34%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3727, Accuracy: 25121/26032 (96.50%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4141, Accuracy: 25108/26032 (96.45%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4237, Accuracy: 25109/26032 (96.45%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5396, Accuracy: 25080/26032 (96.34%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5698, Accuracy: 25056/26032 (96.25%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2379, Accuracy: 25074/26032 (96.32%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3386, Accuracy: 25070/26032 (96.30%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4526, Accuracy: 25088/26032 (96.37%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4172, Accuracy: 25062/26032 (96.27%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5139, Accuracy: 25050/26032 (96.23%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5409, Accuracy: 25084/26032 (96.36%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5990, Accuracy: 25061/26032 (96.27%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6392, Accuracy: 25056/26032 (96.25%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7122, Accuracy: 25053/26032 (96.24%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7273, Accuracy: 25063/26032 (96.28%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4216, Accuracy: 25118/26032 (96.49%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4378, Accuracy: 25123/26032 (96.51%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5104, Accuracy: 25115/26032 (96.48%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5189, Accuracy: 25119/26032 (96.49%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6071, Accuracy: 25108/26032 (96.45%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6198, Accuracy: 25094/26032 (96.40%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6230, Accuracy: 25078/26032 (96.34%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7062, Accuracy: 25095/26032 (96.40%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7378, Accuracy: 25089/26032 (96.38%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7783, Accuracy: 25090/26032 (96.38%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.1371, Accuracy: 25085/26032 (96.36%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.2823, Accuracy: 25092/26032 (96.39%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4025, Accuracy: 25107/26032 (96.45%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4620, Accuracy: 25111/26032 (96.46%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4859, Accuracy: 25087/26032 (96.37%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4781, Accuracy: 25053/26032 (96.24%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5547, Accuracy: 25089/26032 (96.38%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5688, Accuracy: 25088/26032 (96.37%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5485, Accuracy: 25097/26032 (96.41%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6011, Accuracy: 25095/26032 (96.40%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3740, Accuracy: 25068/26032 (96.30%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4615, Accuracy: 25078/26032 (96.34%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5453, Accuracy: 25061/26032 (96.27%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6105, Accuracy: 25074/26032 (96.32%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6428, Accuracy: 25077/26032 (96.33%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6070, Accuracy: 25037/26032 (96.18%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6771, Accuracy: 25068/26032 (96.30%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7010, Accuracy: 25054/26032 (96.24%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7717, Accuracy: 25060/26032 (96.27%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.8518, Accuracy: 25060/26032 (96.27%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.02\n",
      "Test set: Average loss: -8.2571, Accuracy: 25035/26032 (96.17%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.3238, Accuracy: 25036/26032 (96.17%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4041, Accuracy: 25034/26032 (96.17%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4119, Accuracy: 25034/26032 (96.17%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.4795, Accuracy: 25026/26032 (96.14%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5670, Accuracy: 25031/26032 (96.15%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5305, Accuracy: 25018/26032 (96.10%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5831, Accuracy: 25003/26032 (96.05%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6636, Accuracy: 25054/26032 (96.24%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6350, Accuracy: 25032/26032 (96.16%)\n",
      "Epoch: 1:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5152, Accuracy: 25087/26032 (96.37%)\n",
      "Epoch: 2:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5789, Accuracy: 25057/26032 (96.25%)\n",
      "Epoch: 3:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5660, Accuracy: 25054/26032 (96.24%)\n",
      "Epoch: 4:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.5605, Accuracy: 25055/26032 (96.25%)\n",
      "Epoch: 5:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6905, Accuracy: 25068/26032 (96.30%)\n",
      "Epoch: 6:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.6711, Accuracy: 25058/26032 (96.26%)\n",
      "Epoch: 7:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7449, Accuracy: 25052/26032 (96.24%)\n",
      "Epoch: 8:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7353, Accuracy: 25026/26032 (96.14%)\n",
      "Epoch: 9:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.7979, Accuracy: 25032/26032 (96.16%)\n",
      "Epoch: 10:\n",
      "Train Set: Average Loss: 0.01\n",
      "Test set: Average loss: -8.8698, Accuracy: 25048/26032 (96.22%)\n"
     ]
    }
   ],
   "source": [
    "pretrain=100\n",
    "reward=2\n",
    "alpha=[0.5,1.0,1.5,2,4]\n",
    "# model = models.__dict__['vgg16'](num_classes=num_classes, input_size = input_size)\n",
    "# model=Model()\n",
    "for a in alpha:\n",
    "    for i in range(10):\n",
    "        model = VGG(16, 11)#ResNet(18, 11)  #\n",
    "        model.load_state_dict(torch.load(f\"vgg16_svhn_model_{i}.pt\"))\n",
    "        model = model.cuda()\n",
    "\n",
    "        milestones = [10,20,25,30,35, 40,45, 50, 55, 60,  70,  80,90]\n",
    "\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=milestones, gamma=0.5\n",
    "        )\n",
    "\n",
    "        for epoch in range(1, 10 + 1):\n",
    "            torch.cuda.empty_cache()\n",
    "            train(model, train_loader, optimizer, epoch,alpha=a)\n",
    "            torch.cuda.empty_cache()\n",
    "            test(model, test_loader)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        torch.save(model, f\"D:/OneDrive - Deakin University/review paper/data for paper/NWLoss/alpha {a}/NWLoss_vgg16_svhn_model_{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99787516",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'covered_correct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17352/1777857329.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcovered_correct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'covered_correct' is not defined"
     ]
    }
   ],
   "source": [
    "covered_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc23af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG(16, 11)\n",
    "model.load_state_dict(torch.load(f\"vgg16_cifar_model_0.pt\"))\n",
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4144982",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped=[]\n",
    "correct=0\n",
    "for data,label in test_loader:\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        pre_softmax=model(data.cuda())  #\n",
    "        softmax = F.softmax(pre_softmax, dim=1)\n",
    "    softmax=softmax.cpu()            #[:,:-1]\n",
    "    entropy=(softmax*torch.log(softmax)).sum(1)\n",
    "    GI=1-torch.pow(softmax,2).sum(1)\n",
    "\n",
    "    top=(softmax-softmax.max(1)[0].unsqueeze(1)).sum(1).pow(2)\n",
    "    mu_2=(1-softmax.max(1)[0])/(11-1)\n",
    "    down=torch.pow(torch.sort(softmax)[0][:,:-1]-mu_2.unsqueeze(1),2).sum(1)\n",
    "    LDAM=2*top/((11-1)*down)\n",
    "\n",
    "    sorted_softmax=torch.sort(softmax)[0]-softmax.max(1)[0].unsqueeze(1)\n",
    "    J_P=softmax.max(1)[0]-sorted_softmax.var(1)-sorted_softmax.mean(1)\n",
    "    \n",
    "    confidence=1-torch.sort(softmax)[0][:,-2]\n",
    "\n",
    "\n",
    "    sorted_softmax=torch.sort(softmax)[0][:,:]\n",
    "    Relative_diff=1-sorted_softmax[:,-2]/sorted_softmax[:,-1]\n",
    "    \n",
    "    zipped.extend(list(zip(softmax[:,-1],GI,entropy,LDAM,J_P,confidence,Relative_diff,softmax.max(1)[1].eq(label))))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad6653f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0948d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=['extra class ','GI\\t\\t','entropy\\t','LDAM\\t','J_P\\t\\t','confidence\\t','Relativ_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "95ad7ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tcoverage\n",
      "\t\t\t     98\t\t  95\t       90\n",
      "accracy for extra class  24745 96.997 |24220 97.938 |23074 98.485 |\n",
      "\n",
      "accracy for GI\t\t 24758 97.048 |24281 98.184 |23140 98.766 |\n",
      "\n",
      "accracy for entropy\t 24762 97.064 |24278 98.172 |23136 98.749 |\n",
      "\n",
      "accracy for LDAM\t 24751 97.021 |24283 98.192 |23161 98.856 |\n",
      "\n",
      "accracy for J_P\t\t 24758 97.048 |24281 98.184 |23140 98.766 |\n",
      "\n",
      "accracy for confidence\t 24743 96.990 |24283 98.192 |23158 98.843 |\n",
      "\n",
      "accracy for Relativ_diff 24752 97.025 |24282 98.188 |23158 98.843 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#which metric is needed higher value, that should be saved in descending order\n",
    "print('\\t\\t\\t\\t\\tcoverage\\n\\t\\t\\t     98\\t\\t  95\\t       90')\n",
    "for k in range(7):\n",
    "    descending=True\n",
    "    if k<2:\n",
    "        descending=False#True#\n",
    "    reverse=False\n",
    "    if descending:\n",
    "        reverse=True\n",
    "    zipped.sort(key = lambda x: x[k],reverse=reverse)\n",
    "    sorted_correct = list(map(lambda x: int(x[7]), zipped))\n",
    "    size = len(sorted_correct)\n",
    "    print(f'accracy for {name[k]} ',end='')\n",
    "    expected_coverage=[98.,95.,90.]\n",
    "    for coverage in expected_coverage:\n",
    "        covered_correct = sorted_correct[:round(size/100*coverage)]\n",
    "        print('{} {:.3f} |'.format(sum(covered_correct), sum(covered_correct)/len(covered_correct)*100.), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb27d426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcoverage\tcoverage\tcoverage\tcoverage\n",
      "  90:8428 93.644,   90:8428 93.644,   90:8428 93.644,   90:8428 93.644,   90:8428 93.644, "
     ]
    }
   ],
   "source": [
    "print('\\tcoverage\\tcoverage\\tcoverage\\tcoverage')\n",
    "for i in range(5):\n",
    "    print('  {:.0f}:{} {:.3f}, '.format(coverage, sum(covered_correct), sum(covered_correct)/len(covered_correct)*100.), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2477fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "def conv_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "def cfg(depth):\n",
    "    depth_lst = [11, 13, 16, 19]\n",
    "    assert (depth in depth_lst), \"Error : VGGnet depth should be either 11, 13, 16, 19\"\n",
    "    cf_dict = {\n",
    "        '11': [\n",
    "            64, 'mp',\n",
    "            128, 'mp',\n",
    "            256, 256, 'mp',\n",
    "            512, 512, 'mp',\n",
    "            512, 512, 'mp'],\n",
    "        '13': [\n",
    "            64, 64, 'mp',\n",
    "            128, 128, 'mp',\n",
    "            256, 256, 'mp',\n",
    "            512, 512, 'mp',\n",
    "            512, 512, 'mp'\n",
    "            ],\n",
    "        '16': [\n",
    "            64, 64, 'mp',\n",
    "            128, 128, 'mp',\n",
    "            256, 256, 256, 'mp',\n",
    "            512, 512, 512, 'mp',\n",
    "            512, 512, 512, 'mp'\n",
    "            ],\n",
    "        '19': [\n",
    "            64, 64, 'mp',\n",
    "            128, 128, 'mp',\n",
    "            256, 256, 256, 256, 'mp',\n",
    "            512, 512, 512, 512, 'mp',\n",
    "            512, 512, 512, 512, 'mp'\n",
    "            ],\n",
    "    }\n",
    "\n",
    "    return cf_dict[str(depth)]\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, depth, num_classes, dropout_rate = 0.):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg(depth), dropout_rate)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "\n",
    "        #below to work with datasets that have resolution other than 32x32\n",
    "        m = torch.nn.AdaptiveAvgPool2d((1))\n",
    "        out = m(out)\n",
    "        #  --end \n",
    "\n",
    "        #pdb.set_trace()\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg, dropout_rate):\n",
    "        layers = []\n",
    "        in_planes = 3\n",
    "\n",
    "        for x in cfg:\n",
    "            if x == 'mp':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                if dropout_rate > 0.:\n",
    "                    layers += [conv3x3(in_planes, x), nn.BatchNorm2d(x), \n",
    "                        nn.ReLU(inplace=True), nn.Dropout(p=dropout_rate)]\n",
    "                else:\n",
    "                    layers += [conv3x3(in_planes, x), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n",
    "                in_planes = x\n",
    "\n",
    "        # After cfg convolution\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc12c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG(16, 11)\n",
    "y = net(Variable(torch.randn(1,3,32,32)))\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20972904",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('    Total params: %.2fM' % (sum(p.numel() for p in net.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adb06dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "def conv_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "def cfg(depth):\n",
    "    depth_lst = [18, 34, 50, 101, 152]\n",
    "    assert (depth in depth_lst), \"Error : Resnet depth should be either 18, 34, 50, 101, 152\"\n",
    "    cf_dict = {\n",
    "        '18': (BasicBlock, [2,2,2,2]),\n",
    "        '34': (BasicBlock, [3,4,6,3]),\n",
    "        '50': (Bottleneck, [3,4,6,3]),\n",
    "        '101':(Bottleneck, [3,4,23,3]),\n",
    "        '152':(Bottleneck, [3,8,36,3]),\n",
    "    }\n",
    "\n",
    "    return cf_dict[str(depth)]\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=True),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=True),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        block, num_blocks = cfg(depth)\n",
    "\n",
    "        self.conv1 = conv3x3(3,16)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=ResNet(18, 10)\n",
    "y = net(Variable(torch.randn(1,3,32,32)))\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total params: %.2fM' % (sum(p.numel() for p in net.parameters())/1000000.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
