{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef52e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "def get_CIFAR10(root=\"./\"):\n",
    "    input_size = 32\n",
    "    num_classes = 10\n",
    "    normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    \n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root + \"data/CIFAR10\", train=True, transform=train_transform, download=True\n",
    "    )\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root + \"data/CIFAR10\", train=False, transform=test_transform, download=True\n",
    "    )\n",
    "\n",
    "    return input_size, num_classes, train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb00b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SVHN():\n",
    "    dataset = datasets.SVHN\n",
    "    num_classes = 10\n",
    "    input_size = 32\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32,padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    train_dataset = dataset(root='./data', split='train', transform=transform_train,\n",
    "                                     target_transform=None, download=True)\n",
    "    test_dataset = dataset(root='./data', split='test', transform=transform_test,\n",
    "                                   target_transform=None, download=True)\n",
    "    return input_size,num_classes,train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "38541fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "input_size, num_classes, train_dataset, test_dataset = get_CIFAR10()\n",
    "# input_size, num_classes, train_dataset, test_dataset = get_SVHN()\n",
    "\n",
    "kwargs = {\"num_workers\": 2, \"pin_memory\": True}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True, **kwargs\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "edbf428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "def conv_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "def cfg(depth):\n",
    "    depth_lst = [18, 34, 50, 101, 152]\n",
    "    assert (depth in depth_lst), \"Error : Resnet depth should be either 18, 34, 50, 101, 152\"\n",
    "    cf_dict = {\n",
    "        '18': (BasicBlock, [2,2,2,2]),\n",
    "        '34': (BasicBlock, [3,4,6,3]),\n",
    "        '50': (Bottleneck, [3,4,6,3]),\n",
    "        '101':(Bottleneck, [3,4,23,3]),\n",
    "        '152':(Bottleneck, [3,8,36,3]),\n",
    "    }\n",
    "\n",
    "    return cf_dict[str(depth)]\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=True),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=True),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        block, num_blocks = cfg(depth)\n",
    "\n",
    "        self.conv1 = conv3x3(3,16)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "af4c49e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "def conv_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "def cfg(depth):\n",
    "    depth_lst = [11, 13, 16, 19]\n",
    "    assert (depth in depth_lst), \"Error : VGGnet depth should be either 11, 13, 16, 19\"\n",
    "    cf_dict = {\n",
    "        '11': [\n",
    "            64, 'mp',\n",
    "            128, 'mp',\n",
    "            256, 256, 'mp',\n",
    "            512, 512, 'mp',\n",
    "            512, 512, 'mp'],\n",
    "        '13': [\n",
    "            64, 64, 'mp',\n",
    "            128, 128, 'mp',\n",
    "            256, 256, 'mp',\n",
    "            512, 512, 'mp',\n",
    "            512, 512, 'mp'\n",
    "            ],\n",
    "        '16': [\n",
    "            64, 64, 'mp',\n",
    "            128, 128, 'mp',\n",
    "            256, 256, 256, 'mp',\n",
    "            512, 512, 512, 'mp',\n",
    "            512, 512, 512, 'mp'\n",
    "            ],\n",
    "        '19': [\n",
    "            64, 64, 'mp',\n",
    "            128, 128, 'mp',\n",
    "            256, 256, 256, 256, 'mp',\n",
    "            512, 512, 512, 512, 'mp',\n",
    "            512, 512, 512, 512, 'mp'\n",
    "            ],\n",
    "    }\n",
    "\n",
    "    return cf_dict[str(depth)]\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, depth, num_classes, dropout_rate = 0.):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg(depth), dropout_rate)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "\n",
    "        #below to work with datasets that have resolution other than 32x32\n",
    "        m = torch.nn.AdaptiveAvgPool2d((1))\n",
    "        out = m(out)\n",
    "        #  --end \n",
    "\n",
    "        #pdb.set_trace()\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg, dropout_rate):\n",
    "        layers = []\n",
    "        in_planes = 3\n",
    "\n",
    "        for x in cfg:\n",
    "            if x == 'mp':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                if dropout_rate > 0.:\n",
    "                    layers += [conv3x3(in_planes, x), nn.BatchNorm2d(x), \n",
    "                        nn.ReLU(inplace=True), nn.Dropout(p=dropout_rate)]\n",
    "                else:\n",
    "                    layers += [conv3x3(in_planes, x), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n",
    "                in_planes = x\n",
    "\n",
    "        # After cfg convolution\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cb820346",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17076/2883986139.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"resnet18_svhn_model_{i}.pt\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#{path}/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mzipped\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "path=\"D:/OneDrive - Deakin University/review paper/data for paper/NWLoss/alpha 2\"\n",
    "model = VGG(16, 11)#ResNet(18, 11)#\n",
    "result=[]\n",
    "for i in range(10):\n",
    "    model=torch.load(f\"resnet18_svhn_model_{i}.pt\") #{path}/\n",
    "    model=model.cuda()\n",
    "    zipped=[]\n",
    "    for data,label in test_loader:\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            pre_softmax=model(data.cuda())  #\n",
    "            softmax = F.softmax(pre_softmax, dim=1)\n",
    "        softmax=softmax[:,:-1].cpu()            #\n",
    "        entropy=(softmax*torch.log(softmax)).sum(1)\n",
    "        GI=1-torch.pow(softmax,2).sum(1)\n",
    "\n",
    "        top=(softmax-softmax.max(1)[0].unsqueeze(1)).sum(1).pow(2)\n",
    "        mu_2=(1-softmax.max(1)[0])/(10-1)\n",
    "        down=torch.pow(torch.sort(softmax)[0][:,:-1]-mu_2.unsqueeze(1),2).sum(1)\n",
    "        LDAM=2*top/((10-1)*down)\n",
    "\n",
    "        sorted_softmax=torch.sort(softmax)[0]-softmax.max(1)[0].unsqueeze(1)\n",
    "        J_P=softmax.max(1)[0]-sorted_softmax.var(1)-sorted_softmax.mean(1)\n",
    "\n",
    "        confidence=1-torch.sort(softmax)[0][:,-2]\n",
    "\n",
    "\n",
    "        sorted_softmax=torch.sort(softmax)[0][:,:]\n",
    "        Relative_diff=1-sorted_softmax[:,-2]/sorted_softmax[:,-1]\n",
    "\n",
    "        zipped.extend(list(zip(softmax[:,-1],GI,entropy,LDAM,J_P,confidence,Relative_diff,softmax.max(1)[1].eq(label))))\n",
    "\n",
    "\n",
    "    name=['extra class ','GI\\t\\t','entropy\\t','LDAM\\t','J_P\\t\\t','confidence\\t','Relativ_diff']\n",
    "    print('\\t\\t\\t\\t\\tcoverage\\n\\t\\t\\t     98\\t\\t  95\\t       90')\n",
    "    data1=[]\n",
    "    data2=[]\n",
    "    for k in range(7):\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        descending=True\n",
    "        if k<2:\n",
    "            descending=False#True#\n",
    "        reverse=False\n",
    "        if descending:\n",
    "            reverse=True\n",
    "        zipped.sort(key = lambda x: x[k],reverse=reverse)\n",
    "        sorted_correct = list(map(lambda x: int(x[7]), zipped))\n",
    "        size = len(sorted_correct)\n",
    "        print(f'accracy for {name[k]} ',end='')\n",
    "        expected_coverage=[98.,95.,90.]\n",
    "        for coverage in expected_coverage:\n",
    "            covered_correct = sorted_correct[:round(size/100*coverage)]\n",
    "            print('{} {:.3f} |'.format(sum(covered_correct), sum(covered_correct)/len(covered_correct)*100.), end='')\n",
    "            temp1.extend([sum(covered_correct)])\n",
    "            temp2.extend([sum(covered_correct)/len(covered_correct)*100.])\n",
    "        print('\\n')\n",
    "        data1.append(temp1)\n",
    "        data2.append(temp2)\n",
    "    result.append([data1,data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "52bc0de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8967.7002)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=torch.tensor(result)\n",
    "res[:,0,1,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4537c1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[93.0061, 94.1463, 95.7489],\n",
       "         [93.1031, 94.3968, 96.1722],\n",
       "         [93.1194, 94.4053, 96.1844],\n",
       "         [93.0143, 94.3611, 96.1356],\n",
       "         [93.0469, 94.3895, 96.1722],\n",
       "         [92.9388, 94.3463, 96.1200],\n",
       "         [93.0000, 94.3600, 96.1222]]),\n",
       " tensor([[0.2847, 0.2953, 0.2540],\n",
       "         [0.3066, 0.2620, 0.1996],\n",
       "         [0.3157, 0.2538, 0.2064],\n",
       "         [0.2891, 0.2608, 0.1675],\n",
       "         [0.3098, 0.2577, 0.2008],\n",
       "         [0.2893, 0.2803, 0.1775],\n",
       "         [0.2878, 0.2593, 0.1798]]))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn=res.mean(0)\n",
    "std=torch.pow(res.var(0),0.5)\n",
    "mn[0].int(),std[0].int()\n",
    "mn[1],std[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "30ef44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fed51deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.extend(mn[0].int().numpy())\n",
    "d.extend(std[0].int().numpy())\n",
    "d.extend(mn[1].numpy())\n",
    "d.extend(std[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67f5e99",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1360/2305769950.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "26cf7f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(b).to_csv(f'Gambler_4_6_resnet18_svhn.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42a86f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\tcoverage\n",
      "\t\t\t\t98\t\t 95\t       90\n",
      "accracy for extra class  96.85979 +- 0.094428845 97.62394 +- 0.0891682 98.37637 +- 0.05986293 \n",
      "\n",
      "accracy for GI\t\t 97.04205 +- 0.096675485 98.11807 +- 0.06983652 98.95557 +- 0.07290774 \n",
      "\n",
      "accracy for entropy\t 96.995804 +- 0.09280185 98.041245 +- 0.07734083 98.92611 +- 0.077462144 \n",
      "\n",
      "accracy for LDAM\t 97.04755 +- 0.0860356 98.127785 +- 0.084155485 98.96837 +- 0.0755818 \n",
      "\n",
      "accracy for J_P\t\t 97.07969 +- 0.073983334 98.12373 +- 0.06854469 98.95643 +- 0.07229988 \n",
      "\n",
      "accracy for confidence\t 96.93584 +- 0.07526586 98.08451 +- 0.08428515 98.95813 +- 0.07385793 \n",
      "\n",
      "accracy for Relativ_diff 97.03383 +- 0.081305444 98.122116 +- 0.08212722 98.96154 +- 0.075661786 \n",
      "\n",
      "                                   98                95                90\n",
      "extra class          96.860$\\pm$0.094  97.624$\\pm$0.089  98.376$\\pm$0.060\n",
      "GI                   97.042$\\pm$0.097  98.118$\\pm$0.070  98.956$\\pm$0.073\n",
      "Entropy              96.996$\\pm$0.093  98.041$\\pm$0.077  98.926$\\pm$0.077\n",
      "LDAM                 97.048$\\pm$0.086  98.128$\\pm$0.084  98.968$\\pm$0.076\n",
      "$J_P$                97.080$\\pm$0.074  98.124$\\pm$0.069  98.956$\\pm$0.072\n",
      "confidence           96.936$\\pm$0.075  98.085$\\pm$0.084  98.958$\\pm$0.074\n",
      "Relative difference  97.034$\\pm$0.081  98.122$\\pm$0.082  98.962$\\pm$0.076\n",
      "\t\t\t\t\t\tcoverage\n",
      "\t\t\t\t98\t\t 95\t       90\n",
      "accracy for extra class  96.86723 +- 0.15753229 97.61706 +- 0.13159789 98.35332 +- 0.07192284 \n",
      "\n",
      "accracy for GI\t\t 97.06323 +- 0.13683708 98.110794 +- 0.10039408 98.94704 +- 0.06276269 \n",
      "\n",
      "accracy for entropy\t 97.023636 +- 0.1463308 98.03598 +- 0.12068154 98.932945 +- 0.0599241 \n",
      "\n",
      "accracy for LDAM\t 97.06676 +- 0.13745202 98.11403 +- 0.086101055 98.95642 +- 0.07587969 \n",
      "\n",
      "accracy for J_P\t\t 97.06832 +- 0.12978989 98.10958 +- 0.10135668 98.94618 +- 0.06430571 \n",
      "\n",
      "accracy for confidence\t 96.94994 +- 0.1308512 98.08532 +- 0.09122446 98.94917 +- 0.07679603 \n",
      "\n",
      "accracy for Relativ_diff 97.051865 +- 0.1391712 98.10594 +- 0.08580515 98.95173 +- 0.076964885 \n",
      "\n",
      "                                   98                95                90\n",
      "extra class          96.867$\\pm$0.158  97.617$\\pm$0.132  98.353$\\pm$0.072\n",
      "GI                   97.063$\\pm$0.137  98.111$\\pm$0.100  98.947$\\pm$0.063\n",
      "Entropy              97.024$\\pm$0.146  98.036$\\pm$0.121  98.933$\\pm$0.060\n",
      "LDAM                 97.067$\\pm$0.137  98.114$\\pm$0.086  98.956$\\pm$0.076\n",
      "$J_P$                97.068$\\pm$0.130  98.110$\\pm$0.101  98.946$\\pm$0.064\n",
      "confidence           96.950$\\pm$0.131  98.085$\\pm$0.091  98.949$\\pm$0.077\n",
      "Relative difference  97.052$\\pm$0.139  98.106$\\pm$0.086  98.952$\\pm$0.077\n",
      "\t\t\t\t\t\tcoverage\n",
      "\t\t\t\t98\t\t 95\t       90\n",
      "accracy for extra class  96.864105 +- 0.11168326 97.63769 +- 0.0715966 98.37851 +- 0.06408501 \n",
      "\n",
      "accracy for GI\t\t 97.026375 +- 0.115296155 98.08653 +- 0.088217966 98.93038 +- 0.059592392 \n",
      "\n",
      "accracy for entropy\t 96.990715 +- 0.11414993 98.00566 +- 0.08391193 98.902214 +- 0.0643151 \n",
      "\n",
      "accracy for LDAM\t 97.02089 +- 0.11203501 98.07723 +- 0.11172353 98.93935 +- 0.06871184 \n",
      "\n",
      "accracy for J_P\t\t 97.03853 +- 0.10957408 98.08532 +- 0.08886313 98.931244 +- 0.05966079 \n",
      "\n",
      "accracy for confidence\t 96.89899 +- 0.104634136 98.03316 +- 0.11428052 98.93465 +- 0.070513435 \n",
      "\n",
      "accracy for Relativ_diff 96.99972 +- 0.10830393 98.07036 +- 0.11398623 98.938065 +- 0.06772028 \n",
      "\n",
      "                                   98                95                90\n",
      "extra class          96.864$\\pm$0.112  97.638$\\pm$0.072  98.379$\\pm$0.064\n",
      "GI                   97.026$\\pm$0.115  98.087$\\pm$0.088  98.930$\\pm$0.060\n",
      "Entropy              96.991$\\pm$0.114  98.006$\\pm$0.084  98.902$\\pm$0.064\n",
      "LDAM                 97.021$\\pm$0.112  98.077$\\pm$0.112  98.939$\\pm$0.069\n",
      "$J_P$                97.039$\\pm$0.110  98.085$\\pm$0.089  98.931$\\pm$0.060\n",
      "confidence           96.899$\\pm$0.105  98.033$\\pm$0.114  98.935$\\pm$0.071\n",
      "Relative difference  97.000$\\pm$0.108  98.070$\\pm$0.114  98.938$\\pm$0.068\n"
     ]
    }
   ],
   "source": [
    "# file=['gambler_resenet18_svhn.csv','gambler_resenet18_cifar.csv','gambler_vgg16_svhn.csv','gambler_vgg16_cifar.csv']\n",
    "# df=pd.read_csv(file[2])\n",
    "path=[\"D:/OneDrive - Deakin University/review paper/data for paper/DAC\",\n",
    "      \"D:/OneDrive - Deakin University/review paper/data for paper/NWLoss/alpha 2\",\n",
    "      \"D:/OneDrive - Deakin University/review paper/data for paper/gambler/reward 2\"]\n",
    "pre=['DAC','NWLoss','gambler']\n",
    "c=pd.DataFrame()\n",
    "for k in range(3):\n",
    "    df=pd.read_csv(f'{path[k]}/{pre[k]}_resnet18_svhn.csv')\n",
    "\n",
    "    b=[]\n",
    "    name=['extra class ','GI\\t\\t','entropy\\t','LDAM\\t','J_P\\t\\t','confidence\\t','Relativ_diff']\n",
    "    idx=['extra class','GI','Entropy','LDAM','$J_P$','confidence','Relative difference']\n",
    "    print('\\t\\t\\t\\t\\t\\tcoverage\\n\\t\\t\\t\\t98\\t\\t 95\\t       90')\n",
    "    for i in range(7):\n",
    "        print(f'accracy for {name[i]} ',end='')\n",
    "        a=[]\n",
    "        for j in range(3):\n",
    "            print(df.iloc[i+14][j],'+-',df.iloc[(i+21)][j],end=' ')\n",
    "            a.append(\"{:.3f}\".format(df.iloc[i+14][j])+'$\\pm$'+\"{:.3f}\".format(df.iloc[i+21][j]))\n",
    "        b.append(a)\n",
    "        \n",
    "        print('\\n')\n",
    "    b=pd.DataFrame(b,columns=[98,95,90],index=idx)\n",
    "    print(b)\n",
    "    c=pd.concat([c,b], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d4674e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\tcoverage\n",
      "\t\t\t\t98\t\t 95\t       90\n",
      "accracy for extra class  96.8786 +- 0.103661835 97.67206 +- 0.09794291 98.42076 +- 0.052698523 \n",
      "\n",
      "accracy for GI\t\t 97.01854 +- 0.10541085 98.09219 +- 0.09467545 98.92142 +- 0.08517459 \n",
      "\n",
      "accracy for entropy\t 96.97972 +- 0.10544451 98.00809 +- 0.0968076 98.90137 +- 0.08529877 \n",
      "\n",
      "accracy for LDAM\t 97.037346 +- 0.11295872 98.08208 +- 0.107155085 98.93423 +- 0.0825202 \n",
      "\n",
      "accracy for J_P\t\t 97.045586 +- 0.10724039 98.09624 +- 0.09754865 98.923134 +- 0.08581563 \n",
      "\n",
      "accracy for confidence\t 96.91584 +- 0.12336026 98.04933 +- 0.10828774 98.92825 +- 0.08427679 \n",
      "\n",
      "accracy for Relativ_diff 97.02442 +- 0.11236334 98.07885 +- 0.10533298 98.93252 +- 0.08104442 \n",
      "\n",
      "                                   98                95                90\n",
      "extra class          96.879$\\pm$0.104  97.672$\\pm$0.098  98.421$\\pm$0.053\n",
      "GI                   97.019$\\pm$0.105  98.092$\\pm$0.095  98.921$\\pm$0.085\n",
      "Entropy              96.980$\\pm$0.105  98.008$\\pm$0.097  98.901$\\pm$0.085\n",
      "LDAM                 97.037$\\pm$0.113  98.082$\\pm$0.107  98.934$\\pm$0.083\n",
      "$J_P$                97.046$\\pm$0.107  98.096$\\pm$0.098  98.923$\\pm$0.086\n",
      "confidence           96.916$\\pm$0.123  98.049$\\pm$0.108  98.928$\\pm$0.084\n",
      "Relative difference  97.024$\\pm$0.112  98.079$\\pm$0.105  98.933$\\pm$0.081\n",
      "\t\t\t\t\t\tcoverage\n",
      "\t\t\t\t98\t\t 95\t       90\n",
      "accracy for extra class  96.888405 +- 0.122165486 97.66316 +- 0.08662974 98.39813 +- 0.050823253 \n",
      "\n",
      "accracy for GI\t\t 97.067924 +- 0.104530334 98.10674 +- 0.083696015 98.92953 +- 0.05754024 \n",
      "\n",
      "accracy for entropy\t 97.01814 +- 0.11471629 98.03882 +- 0.09716087 98.90648 +- 0.056113888 \n",
      "\n",
      "accracy for LDAM\t 97.04716 +- 0.10188461 98.10474 +- 0.09110156 98.935936 +- 0.060194872 \n",
      "\n",
      "accracy for J_P\t\t 97.06989 +- 0.09768012 98.11039 +- 0.08924777 98.93038 +- 0.057378348 \n",
      "\n",
      "accracy for confidence\t 96.93113 +- 0.12643135 98.06673 +- 0.092494324 98.934654 +- 0.059795372 \n",
      "\n",
      "accracy for Relativ_diff 97.03539 +- 0.10586163 98.098656 +- 0.095743924 98.93636 +- 0.060489412 \n",
      "\n",
      "                                   98                95                90\n",
      "extra class          96.888$\\pm$0.122  97.663$\\pm$0.087  98.398$\\pm$0.051\n",
      "GI                   97.068$\\pm$0.105  98.107$\\pm$0.084  98.930$\\pm$0.058\n",
      "Entropy              97.018$\\pm$0.115  98.039$\\pm$0.097  98.906$\\pm$0.056\n",
      "LDAM                 97.047$\\pm$0.102  98.105$\\pm$0.091  98.936$\\pm$0.060\n",
      "$J_P$                97.070$\\pm$0.098  98.110$\\pm$0.089  98.930$\\pm$0.057\n",
      "confidence           96.931$\\pm$0.126  98.067$\\pm$0.092  98.935$\\pm$0.060\n",
      "Relative difference  97.035$\\pm$0.106  98.099$\\pm$0.096  98.936$\\pm$0.060\n",
      "\t\t\t\t\t\tcoverage\n",
      "\t\t\t\t98\t\t 95\t       90\n",
      "accracy for extra class  96.85666 +- 0.104814574 97.60372 +- 0.090683125 98.3593 +- 0.08463032 \n",
      "\n",
      "accracy for GI\t\t 97.04285 +- 0.08755636 98.097046 +- 0.08650146 98.912025 +- 0.04150075 \n",
      "\n",
      "accracy for entropy\t 97.006775 +- 0.08990501 98.00485 +- 0.08486594 98.89154 +- 0.035769034 \n",
      "\n",
      "accracy for LDAM\t 97.041664 +- 0.09016766 98.09664 +- 0.085589916 98.92356 +- 0.049521796 \n",
      "\n",
      "accracy for J_P\t\t 97.06205 +- 0.09715349 98.10472 +- 0.08107813 98.91074 +- 0.042956106 \n",
      "\n",
      "accracy for confidence\t 96.93349 +- 0.10298469 98.05742 +- 0.0934658 98.91758 +- 0.056525305 \n",
      "\n",
      "accracy for Relativ_diff 97.02794 +- 0.09340058 98.08977 +- 0.08569906 98.91929 +- 0.05368089 \n",
      "\n",
      "                                   98                95                90\n",
      "extra class          96.857$\\pm$0.105  97.604$\\pm$0.091  98.359$\\pm$0.085\n",
      "GI                   97.043$\\pm$0.088  98.097$\\pm$0.087  98.912$\\pm$0.042\n",
      "Entropy              97.007$\\pm$0.090  98.005$\\pm$0.085  98.892$\\pm$0.036\n",
      "LDAM                 97.042$\\pm$0.090  98.097$\\pm$0.086  98.924$\\pm$0.050\n",
      "$J_P$                97.062$\\pm$0.097  98.105$\\pm$0.081  98.911$\\pm$0.043\n",
      "confidence           96.933$\\pm$0.103  98.057$\\pm$0.093  98.918$\\pm$0.057\n",
      "Relative difference  97.028$\\pm$0.093  98.090$\\pm$0.086  98.919$\\pm$0.054\n"
     ]
    }
   ],
   "source": [
    "# path=[\"D:/OneDrive - Deakin University/review paper/data for paper/DAC\",\n",
    "#       \"D:/OneDrive - Deakin University/review paper/data for paper/NWLoss/alpha 2\",\n",
    "#       \"D:/OneDrive - Deakin University/review paper/data for paper/gambler/reward 2\"]\n",
    "path=[\"D:/OneDrive - Deakin University/review paper/data for paper/NWLoss/alpha 0.5\",\n",
    "      \"D:/OneDrive - Deakin University/review paper/data for paper/NWLoss/alpha 1.0\",\n",
    "      \"D:/OneDrive - Deakin University/review paper/data for paper/NWLoss/alpha 1.5\",\n",
    "     \"D:/OneDrive - Deakin University/review paper/data for paper/NWLoss/alpha 2\"]\n",
    "pre='NWLoss'#['DAC','NWLoss','gambler']\n",
    "c=pd.DataFrame()\n",
    "for k in range(3):\n",
    "    df=pd.read_csv(f'{path[k]}/{pre}_resnet18_svhn.csv')\n",
    "\n",
    "    b=[]\n",
    "    name=['extra class ','GI\\t\\t','entropy\\t','LDAM\\t','J_P\\t\\t','confidence\\t','Relativ_diff']\n",
    "    idx=['extra class','GI','Entropy','LDAM','$J_P$','confidence','Relative difference']\n",
    "    print('\\t\\t\\t\\t\\t\\tcoverage\\n\\t\\t\\t\\t98\\t\\t 95\\t       90')\n",
    "    for i in range(7):\n",
    "        print(f'accracy for {name[i]} ',end='')\n",
    "        a=[]\n",
    "        for j in range(3):\n",
    "            print(df.iloc[i+14][j],'+-',df.iloc[(i+21)][j],end=' ')\n",
    "            a.append(\"{:.3f}\".format(df.iloc[i+14][j])+'$\\pm$'+\"{:.3f}\".format(df.iloc[i+21][j]))\n",
    "        b.append(a)\n",
    "        \n",
    "        print('\\n')\n",
    "    b=pd.DataFrame(b,columns=[98,95,90],index=idx)\n",
    "    print(b)\n",
    "    c=pd.concat([c,b], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "31257b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.to_csv(f'NWLoss_1_15_resnet18_svhn.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82443473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>98</th>\n",
       "      <th>95</th>\n",
       "      <th>90</th>\n",
       "      <th>98</th>\n",
       "      <th>95</th>\n",
       "      <th>90</th>\n",
       "      <th>98</th>\n",
       "      <th>95</th>\n",
       "      <th>90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>extra class</th>\n",
       "      <td>96.879$\\pm$0.104</td>\n",
       "      <td>97.672$\\pm$0.098</td>\n",
       "      <td>98.421$\\pm$0.053</td>\n",
       "      <td>96.888$\\pm$0.122</td>\n",
       "      <td>97.663$\\pm$0.087</td>\n",
       "      <td>98.398$\\pm$0.051</td>\n",
       "      <td>96.857$\\pm$0.105</td>\n",
       "      <td>97.604$\\pm$0.091</td>\n",
       "      <td>98.359$\\pm$0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GI</th>\n",
       "      <td>97.019$\\pm$0.105</td>\n",
       "      <td>98.092$\\pm$0.095</td>\n",
       "      <td>98.921$\\pm$0.085</td>\n",
       "      <td>97.068$\\pm$0.105</td>\n",
       "      <td>98.107$\\pm$0.084</td>\n",
       "      <td>98.930$\\pm$0.058</td>\n",
       "      <td>97.043$\\pm$0.088</td>\n",
       "      <td>98.097$\\pm$0.087</td>\n",
       "      <td>98.912$\\pm$0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entropy</th>\n",
       "      <td>96.980$\\pm$0.105</td>\n",
       "      <td>98.008$\\pm$0.097</td>\n",
       "      <td>98.901$\\pm$0.085</td>\n",
       "      <td>97.018$\\pm$0.115</td>\n",
       "      <td>98.039$\\pm$0.097</td>\n",
       "      <td>98.906$\\pm$0.056</td>\n",
       "      <td>97.007$\\pm$0.090</td>\n",
       "      <td>98.005$\\pm$0.085</td>\n",
       "      <td>98.892$\\pm$0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDAM</th>\n",
       "      <td>97.037$\\pm$0.113</td>\n",
       "      <td>98.082$\\pm$0.107</td>\n",
       "      <td>98.934$\\pm$0.083</td>\n",
       "      <td>97.047$\\pm$0.102</td>\n",
       "      <td>98.105$\\pm$0.091</td>\n",
       "      <td>98.936$\\pm$0.060</td>\n",
       "      <td>97.042$\\pm$0.090</td>\n",
       "      <td>98.097$\\pm$0.086</td>\n",
       "      <td>98.924$\\pm$0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$J_P$</th>\n",
       "      <td>97.046$\\pm$0.107</td>\n",
       "      <td>98.096$\\pm$0.098</td>\n",
       "      <td>98.923$\\pm$0.086</td>\n",
       "      <td>97.070$\\pm$0.098</td>\n",
       "      <td>98.110$\\pm$0.089</td>\n",
       "      <td>98.930$\\pm$0.057</td>\n",
       "      <td>97.062$\\pm$0.097</td>\n",
       "      <td>98.105$\\pm$0.081</td>\n",
       "      <td>98.911$\\pm$0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidence</th>\n",
       "      <td>96.916$\\pm$0.123</td>\n",
       "      <td>98.049$\\pm$0.108</td>\n",
       "      <td>98.928$\\pm$0.084</td>\n",
       "      <td>96.931$\\pm$0.126</td>\n",
       "      <td>98.067$\\pm$0.092</td>\n",
       "      <td>98.935$\\pm$0.060</td>\n",
       "      <td>96.933$\\pm$0.103</td>\n",
       "      <td>98.057$\\pm$0.093</td>\n",
       "      <td>98.918$\\pm$0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relative difference</th>\n",
       "      <td>97.024$\\pm$0.112</td>\n",
       "      <td>98.079$\\pm$0.105</td>\n",
       "      <td>98.933$\\pm$0.081</td>\n",
       "      <td>97.035$\\pm$0.106</td>\n",
       "      <td>98.099$\\pm$0.096</td>\n",
       "      <td>98.936$\\pm$0.060</td>\n",
       "      <td>97.028$\\pm$0.093</td>\n",
       "      <td>98.090$\\pm$0.086</td>\n",
       "      <td>98.919$\\pm$0.054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   98                95                90  \\\n",
       "extra class          96.879$\\pm$0.104  97.672$\\pm$0.098  98.421$\\pm$0.053   \n",
       "GI                   97.019$\\pm$0.105  98.092$\\pm$0.095  98.921$\\pm$0.085   \n",
       "Entropy              96.980$\\pm$0.105  98.008$\\pm$0.097  98.901$\\pm$0.085   \n",
       "LDAM                 97.037$\\pm$0.113  98.082$\\pm$0.107  98.934$\\pm$0.083   \n",
       "$J_P$                97.046$\\pm$0.107  98.096$\\pm$0.098  98.923$\\pm$0.086   \n",
       "confidence           96.916$\\pm$0.123  98.049$\\pm$0.108  98.928$\\pm$0.084   \n",
       "Relative difference  97.024$\\pm$0.112  98.079$\\pm$0.105  98.933$\\pm$0.081   \n",
       "\n",
       "                                   98                95                90  \\\n",
       "extra class          96.888$\\pm$0.122  97.663$\\pm$0.087  98.398$\\pm$0.051   \n",
       "GI                   97.068$\\pm$0.105  98.107$\\pm$0.084  98.930$\\pm$0.058   \n",
       "Entropy              97.018$\\pm$0.115  98.039$\\pm$0.097  98.906$\\pm$0.056   \n",
       "LDAM                 97.047$\\pm$0.102  98.105$\\pm$0.091  98.936$\\pm$0.060   \n",
       "$J_P$                97.070$\\pm$0.098  98.110$\\pm$0.089  98.930$\\pm$0.057   \n",
       "confidence           96.931$\\pm$0.126  98.067$\\pm$0.092  98.935$\\pm$0.060   \n",
       "Relative difference  97.035$\\pm$0.106  98.099$\\pm$0.096  98.936$\\pm$0.060   \n",
       "\n",
       "                                   98                95                90  \n",
       "extra class          96.857$\\pm$0.105  97.604$\\pm$0.091  98.359$\\pm$0.085  \n",
       "GI                   97.043$\\pm$0.088  98.097$\\pm$0.087  98.912$\\pm$0.042  \n",
       "Entropy              97.007$\\pm$0.090  98.005$\\pm$0.085  98.892$\\pm$0.036  \n",
       "LDAM                 97.042$\\pm$0.090  98.097$\\pm$0.086  98.924$\\pm$0.050  \n",
       "$J_P$                97.062$\\pm$0.097  98.105$\\pm$0.081  98.911$\\pm$0.043  \n",
       "confidence           96.933$\\pm$0.103  98.057$\\pm$0.093  98.918$\\pm$0.057  \n",
       "Relative difference  97.028$\\pm$0.093  98.090$\\pm$0.086  98.919$\\pm$0.054  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c\n",
    "# \"{:.2f}\".format(df.iloc[14][0])+'$\\pm$'+\"{:.2f}\".format(df.iloc[21][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=['gambler_resenet18_svhn.csv','gambler_resenet18_cifar.csv','gambler_vgg16_svhn.csv','gambler_vgg16_cifar.csv']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
